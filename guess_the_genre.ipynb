{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projet : Traitement Automatique des Langues (Partie 1)\n",
        "\n",
        "Nous avons √† notre disposition deux fichiers CSV ([allocine_genres_test.csv](data/allocine_genres_test.csv) et [allocine_genres_train.csv](data/allocine_genres_train.csv)) contenant des informations sur des films et leurs genres. Le but de ce projet est de pr√©dire les genres d'un film √† partir de son synopsis notamment (et d'autres informations).\n",
        "\n",
        "L‚Äôobjectif est d‚Äôentra√Æner un outil de classification automatique des films en fonction de leur genre. La classification doit se baser sur le texte de la synopsis et sur le titre des films. Le texte et le titre des articles ont d√©j√† √©t√© tokenis√©s et tous les tokens sont s√©par√©s par un espace."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importation des donn√©es et analyse exploratoire\n",
        "\n",
        "Les donn√©es sont disponibles dans le dossier [data](data/). Nous allons commencer par importer les donn√©es et les analyser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# ex√©cuter cette cellule pour installer les d√©pendances et t√©l√©charger les mod√®les spacy\n",
        "# remplacer `python` par `python3` si n√©cessaire\n",
        "# !conda create -n nlp python=3.11\n",
        "# !conda activate nlp\n",
        "!python -m pip install --upgrade -r requirements.txt\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import FrenchStemmer\n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_train.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train.shape)\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train['genre'].value_counts(), end='\\n\\n')\n",
        "print(random.choice(train['synopsis'].unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque d√©j√† que les donn√©s chiffr√©es sont soit des entiers soit des flottants. Les donn√©es textuelles sont des cha√Ænes de caract√®res. Certaines donn√©es sont manquantes : `NaN` dans le cas des donn√©es chiffr√©es et une cha√Æne vide dans le cas des donn√©es textuelles.\n",
        "\n",
        "Pour traiter les donn√©es manquantes, nous avons deux solutions :\n",
        "\n",
        "1. Dans le cas o√π il y a tr√®s peu de donn√©es manquantes, on peut simplement supprimer les entr√©es qui contiennent ces donn√©es manquantes. Cela peut √™tre acceptable si le nombre de donn√©es manquantes est tr√®s faible par rapport √† la taille de l'ensemble de donn√©es et que la suppression de ces entr√©es n'affecte pas significativement les r√©sultats de l'analyse.\n",
        "2. En revanche, si le nombre de donn√©es manquantes est important, la suppression de ces entr√©es pourrait entra√Æner une perte d'informations importantes pour l'analyse. Dans ce cas, il est g√©n√©ralement pr√©f√©rable de remplacer les valeurs manquantes par une valeur qui repr√©sente au mieux l'information manquante. Par exemple, si les donn√©es manquantes sont des scores au box-office, vous pouvez remplacer ces donn√©es manquantes par la moyenne ou la m√©diane des scores de box-office disponibles dans les donn√©es.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EMPTY_TOKEN = '<EMPTY>'\n",
        "\n",
        "# remove rows with missing values\n",
        "train_dataset_1 = train.dropna(how='any', inplace=False)\n",
        "\n",
        "# replace missing values with either the mean or the median (or empty token)\n",
        "list_headers = train.columns.values.tolist()\n",
        "list_of_numerical_headers = train._get_numeric_data().columns.values.tolist()\n",
        "list_of_categorical_headers = list(set(list_headers) - set(list_of_numerical_headers))\n",
        "\n",
        "train_dataset_2 = train.copy()\n",
        "for header in list_of_numerical_headers:\n",
        "  train_dataset_2[header].fillna(train_dataset_2[header].median(), inplace=True)\n",
        "for header in list_of_categorical_headers:\n",
        "  train_dataset_2[header].fillna(EMPTY_TOKEN, inplace=True)\n",
        "\n",
        "print(train_dataset_1.shape)\n",
        "print(train_dataset_2.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lorsqu'on supprime simplement toutes les entr√©es o√π il manque au moins une valeur, on se retrouve uniquement avec 891 valeurs en tout. Cela signifie que nous avons perdu beaucoup d'informations. Nous allons donc utiliser la deuxi√®me solution (au moins dans un premier temps) et remplacer les valeurs manquantes par des valeurs qui repr√©sentent au mieux l'information manquante."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pr√©traitement des donn√©es\n",
        "\n",
        "Il faut aussi corriger les entr√©es textuelles, ainsi qu'appliquer un certain nombre d'algorithmes de pr√©traitement comme : la suppression des caract√®res sp√©ciaux, la suppression des stop words, la suppression des mots trop fr√©quents ou trop rares, la lemmatisation, la suppression des mots trop longs, etc.\n",
        "\n",
        "On d√©finit donc un ensemble de fonctions et de filtres qui vont nous permettre de pr√©traiter les donn√©es textuelles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)        # download the stopwords corpus\n",
        "nlp = spacy.load('fr_core_news_sm')           # load the French model\n",
        "fr_stopwords = set(stopwords.words('french')) # so that `in` tests are faster\n",
        "stemmer = FrenchStemmer()                     # for stemming words\n",
        "\n",
        "\n",
        "# get the tokens of a sentence (word based tokenization)\n",
        "def get_tokens_words(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [w.text for w in doc]\n",
        "\n",
        "\n",
        "# remove stopwords from a sentence\n",
        "def clean_sentence(text: str) -> list[str]:\n",
        "  clean_words: list[str] = []\n",
        "  for token in get_tokens_words(text):\n",
        "    if token not in fr_stopwords:\n",
        "      clean_words.append(token)\n",
        "  return clean_words\n",
        "\n",
        "\n",
        "# get the tokens of multiple sentences (sentence based tokenization)\n",
        "def get_tokens_sentences(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [s.text for s in doc.sents]\n",
        "\n",
        "\n",
        "# get the lemmas of a sentence\n",
        "def get_stem(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [stemmer.stem(w.text) for w in doc]\n",
        "\n",
        "\n",
        "# get the named entities of a sentence\n",
        "def get_ner(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "\n",
        "# render the named entities of a sentence in a Jupyter notebook\n",
        "def render_ner(text: str) -> None:\n",
        "  doc = nlp(text)\n",
        "  displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "\n",
        "# get the part of speech of a sentence\n",
        "def get_pos(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [(token, token.pos_) for token in doc]\n",
        "\n",
        "\n",
        "# render the part of speech of a sentence in a Jupyter notebook\n",
        "def render_pos(text: str) -> None:\n",
        "  doc = nlp(text)\n",
        "  displacy.render(doc, style='dep', options={'distance': 90})\n",
        "\n",
        "\n",
        "# get the word embeddings of a sentence\n",
        "def get_word_embeddings(text: str) -> list[np.ndarray]:\n",
        "  doc = nlp(text)\n",
        "  return [token.vector for token in doc]\n",
        "\n",
        "\n",
        "# get the similarity between two sentences\n",
        "def get_mean_embedding(text1: str, text2: str) -> float:\n",
        "  doc1 = nlp(text1)\n",
        "  doc2 = nlp(text2)\n",
        "  mean1 = np.mean([token.vector for token in doc1], axis=0)\n",
        "  mean2 = np.mean([token.vector for token in doc2], axis=0)\n",
        "\n",
        "  return np.dot(mean1, mean2) / (np.linalg.norm(mean1) * np.linalg.norm(mean2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Juste pour clarifier les choses, nous allons simplement effectuer des tests sur l'ensemble de phrases suivantes :\n",
        "\n",
        "1. \"Le r√©seau sera bient√¥t r√©tabli √† Marseille\"\n",
        "2. \"La panne r√©seau affecte plusieurs utilisateurs de l'op√©rateur\"\n",
        "3. \"Il fait 18 degr√©s ici\"\n",
        "4. \"Bouygues a eu une coupure de r√©seau √† Marseille. La panne a affect√© 300.000 utilisateurs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1 = 'Le r√©seau sera bient√¥t r√©tabli √† Marseille.'\n",
        "text2 = 'La panne r√©seau affecte plusieurs utilisateurs de l\\'op√©rateur'\n",
        "text3 = 'Il fait 18 degr√©s ici'\n",
        "text4 = 'Bouygues a eu une coupure de r√©seau √† Marseille. La panne a affect√© 300.000 utilisateurs.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# basic tokenization\n",
        "# we can observe `get_tokens_sentences` do not \"cut\" at each . or ! or ?\n",
        "\n",
        "print(get_tokens_words(text1))\n",
        "print(clean_sentence(text1))\n",
        "print(get_tokens_sentences(text4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stemming\n",
        "# this doesn't work very well for French...\n",
        "\n",
        "print(get_stem(text1))\n",
        "print(get_stem(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# named entities recognition\n",
        "\n",
        "print(get_ner(text4))\n",
        "render_ner(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# part of speech\n",
        "\n",
        "print(get_pos(text1))\n",
        "render_pos(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# word embeddings and mean embedding (similarity)\n",
        "\n",
        "print(get_word_embeddings(text1)[0].shape)\n",
        "print(get_mean_embedding(text1, text2))\n",
        "print(get_mean_embedding(text1, text4))\n",
        "print(get_mean_embedding(text2, text4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Pr√©paration des donn√©es\n",
        "\n",
        "Nous allons maintenant pr√©parer les donn√©es pour l'entra√Ænement de notre mod√®le.\n",
        "\n",
        "Nous allons donc appliquer les fonctions de pr√©traitement sur les donn√©es textuelles et transformer les donn√©es chiffr√©es en donn√©es num√©riques. Dans un premier temps, nous confectionnerons des ensembles contenant toutes les valeurs et les informations pr√©sentes dans les donn√©es. Nous verrons par la suites lesquelles sont les plus pertinantes en fonction des r√©sultats obtenus et de nos mod√®les."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we will use train_dataset_2 since it has no missing values\n",
        "\n",
        "values = ['synopsis', 'titre']\n",
        "X = train_dataset_2[values]\n",
        "y = train_dataset_2['genre']\n",
        "\n",
        "print(X.shape)\n",
        "print(X.head())\n",
        "print(y.shape)\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate the dataset into a training set and a validation set\n",
        "X_train, y_train = X, y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour la suite, nous d√©finirons des pipelines de traitement sp√©cifiques √† chaque type de colonne. En particulier, les colonnes correspondant √† des textes (`list_of_categorical_headers`) dans `X` seront vectoris√©es.\n",
        "\n",
        "Pour toutes les donn√©es dans `list_of_categorical_headers`, les tokens ont d√©j√† √©t√© s√©par√©s par des espaces. Nous allons utiliser `TfidfVectorizer` pour vectoriser ces donn√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '),\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  min_df=0.01,\n",
        "  max_df=0.95,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = tfidf_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous pouvons aussi utiliser ce `vectorizer` pour extraire des statistiques sur les donn√©es textuelles. Par exemple, la longueur en nombre de caract√®res, le nombre de phrases, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_stats(texts: list[str]) -> list[dict[str, int]]:\n",
        "  return [{\n",
        "    'len': len(t),\n",
        "    'nb_sentences': t.count('.') + t.count('!') + t.count('?'),\n",
        "  } for t in texts]\n",
        "\n",
        "stats_transformer = FunctionTransformer(make_stats, validate=False)\n",
        "stats_vectorizer = DictVectorizer(sparse=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = stats_vectorizer.fit_transform(stats_transformer.transform(X_train['synopsis']))\n",
        "stats = pd.DataFrame(res, columns=stats_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(stats.shape)\n",
        "print(stats.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On normalise les donn√©es en utilisant `MinMaxScaler` pour les donn√©es dans notre dictionnaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "scaled_stats = pd.DataFrame(min_max_scaler.fit_transform(stats), columns=stats.columns)\n",
        "\n",
        "print(scaled_stats.shape)\n",
        "print(scaled_stats.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On rajoute aussi `CountVectorizer` ainsi que `HashingVectorizer` pour voir si les r√©sultats sont meilleurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_vectorizer = CountVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '),\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  min_df=0.01,\n",
        "  max_df=0.95,\n",
        ")\n",
        "hashing_vectorizer = HashingVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '),\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  n_features=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = count_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = hashing_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=[f'feature_{i}' for i in range(1000)])\n",
        "\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Cr√©ation de la pipeline\n",
        "\n",
        "Nous allons maintenant proc√©der √† la cr√©ation de la pipeline en combinant les cha√Ænes de pr√©-traitement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_transformer = ColumnTransformer(\n",
        "  [\n",
        "    # 'synopsis' column : tf-idf vectorization\n",
        "    ('synopsis', tfidf_vectorizer, 'synopsis'),\n",
        "    # 'titre' column : tf-idf vectorization\n",
        "    ('titre', tfidf_vectorizer, 'titre'),\n",
        "\n",
        "    # 'synopsis' column : stats\n",
        "    ('synopsis_stats', Pipeline([\n",
        "      ('stats_transformer', stats_transformer),\n",
        "      ('stats_vectorizer', stats_vectorizer),\n",
        "      ('min_max_scaler', min_max_scaler),\n",
        "    ]), 'synopsis'),\n",
        "\n",
        "  ],\n",
        "  remainder='drop', # drop the columns not specified\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learning\n",
        "classifier_pipeline = make_pipeline(column_transformer, LogisticRegression(max_iter=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit\n",
        "classifier_pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict\n",
        "test: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_test.csv'))\n",
        "\n",
        "X_test = test[values]\n",
        "y_test = test['genre']\n",
        "\n",
        "y_pred = classifier_pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validation crois√©e\n",
        "\n",
        "Nous allons maintenant proc√©der √† la validation crois√©e de notre mod√®le. Nous allons utiliser `GridSearchCV` pour tester plusieurs hyperparam√®tres et choisir les meilleurs.\n",
        "\n",
        "Les imports n√©cessaires sont effectu√©s dans la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "models = [\n",
        "  ('Baseline', DummyClassifier(strategy='most_frequent')),\n",
        "  ('Mutinomial NB', MultinomialNB()),\n",
        "  ('CART', DecisionTreeClassifier()),\n",
        "  ('LR', LogisticRegression()),\n",
        "  ('KNN', KNeighborsClassifier()),\n",
        "  ('Random forest', RandomForestClassifier()),\n",
        "]\n",
        "\n",
        "# do cross validation\n",
        "for name, model in models:\n",
        "  print(name)\n",
        "  pipeline = make_pipeline(column_transformer, model)\n",
        "  scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1_macro')\n",
        "  print(f'  {scores.mean():.3f} +/- {scores.std():.3f}')\n",
        "\n",
        "# do grid search on the best model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "  'logisticregression__C': [0.1, 1, 10],\n",
        "  'logisticregression__solver': ['lbfgs', 'liblinear'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(classifier_pipeline, param_grid, cv=5, scoring='f1_macro')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "print(grid_search.cv_results_)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Passage sur les transformers\n",
        "\n",
        "Nous allons maintenant explorer la piste des transformers. Pour des raisons de reproductibilit√©, nous allons utiliser des mod√®les pr√©-entra√Æn√©s. Tous les imports n√©cessaires seront effectu√©s sur les cellules suivantes.\n",
        "\n",
        "Nous allons utiliser le mod√®le `xlm-roberta-large-xnli` pour la classification de nos donn√©es. Nous avons besoin pour cela de nouveaux packages :\n",
        "\n",
        "- `transformers`\n",
        "- `torch`\n",
        "- `protobuf`\n",
        "- `sentencepiece`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tbyr/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "uri = 'BaptisteDoyen/camembert-base-xnli'\n",
        "tokenizer = AutoTokenizer.from_pretrained(uri)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(uri).to(device)\n",
        "\n",
        "train: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_train.csv'))\n",
        "\n",
        "classifier = pipeline('zero-shot-classification', model=model, tokenizer=tokenizer)\n",
        "\n",
        "candidate_labels: list[str] = train['genre'].unique().tolist()\n",
        "hypothesis_template: str = \"Ce film est du genre {}.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_genre(text: str) -> str:\n",
        "  result = classifier(text, candidate_labels, hypothesis_template=hypothesis_template)\n",
        "  return result['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 22)\n"
          ]
        }
      ],
      "source": [
        "EMPTY_TOKEN = '<EMPTY>'\n",
        "\n",
        "# replace missing values with either the mean or the median (or empty token)\n",
        "list_headers = train.columns.values.tolist()\n",
        "list_of_numerical_headers = train._get_numeric_data().columns.values.tolist()\n",
        "list_of_categorical_headers = list(set(list_headers) - set(list_of_numerical_headers))\n",
        "\n",
        "train_dataset_2 = train.copy()\n",
        "for header in list_of_numerical_headers:\n",
        "  train_dataset_2[header].fillna(train_dataset_2[header].median(), inplace=True)\n",
        "for header in list_of_categorical_headers:\n",
        "  train_dataset_2[header].fillna(EMPTY_TOKEN, inplace=True)\n",
        "\n",
        "print(train_dataset_2.shape)\n",
        "X_train, y_train = train_dataset_2['synopsis'], train_dataset_2['genre']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875,)\n",
            "(719,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 719/719 [09:08<00:00,  1.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# make model\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tqdm\n",
        "\n",
        "class GenrePredictor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, classifier):\n",
        "    self.classifier = classifier # pipeline\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    return self # nothing else to do\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [predict_genre(x) for x in tqdm.tqdm(X)]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./results',          # output directory\n",
        "  num_train_epochs=1,              # total number of training epochs\n",
        "  per_device_train_batch_size=16,  # batch size per device during training\n",
        "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01,               # strength of weight decay\n",
        "  logging_dir='./logs',            # directory for storing logs\n",
        "  logging_steps=10,\n",
        ")\n",
        "\n",
        "# todo: make train and eval datasets\n",
        "trainer = Trainer(\n",
        "  model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "  args=training_args,                  # training arguments, defined above\n",
        "  train_dataset=train_dataset_2,       # training dataset\n",
        "  eval_dataset=train_dataset_2,        # evaluation dataset\n",
        ")\n",
        "\n",
        "# learning\n",
        "classifier_pipeline = make_pipeline(GenrePredictor(classifier))\n",
        "\n",
        "# fit\n",
        "print(X_train.shape)\n",
        "classifier_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "test: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_test.csv'))\n",
        "\n",
        "X_test = test['synopsis']\n",
        "y_test = test['genre']\n",
        "\n",
        "print(X_test.shape)\n",
        "y_pred = classifier_pipeline.predict(X_test) # takes about 8 minutes (i9-13900h, 32GB RAM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0      Gilbert Grape vit √† Endora dans l' Iowa , avec...\n",
            "1      Aventure √† la fois complexe et myst√©rieuse sur...\n",
            "2      A la mort de sa m√®re , Anne fait une d√©couvert...\n",
            "3      Christiane , une jeune berlinoise de treize an...\n",
            "4      Apr√®s son voyage mouvement√© entre pass√© , pr√©s...\n",
            "                             ...                        \n",
            "714    Un jeune t√©nor du barreau gagne toutes les cau...\n",
            "715    Un biopic du Pr√©sident des Etats-Unis Theodore...\n",
            "716    Deux astronautes , le Lieutenant Payton et le ...\n",
            "717    L' histoire vraie d' une avocate d√©fendant une...\n",
            "718    A Casablanca , pendant la Seconde Guerre mondi...\n",
            "Name: synopsis, Length: 719, dtype: object\n",
            "['historique', 'historique', 'historique', 'drame', 'historique', 'drame', 'drame', 'policier', 'romance', 'com√©die', 'historique', 'historique', 'drame', 'com√©die', 'historique', 'policier', 'historique', 'policier', 'policier', 'drame', 'historique', 'documentaire', 'horreur', 'historique', 'com√©die', 'policier', 'drame', 'policier', 'historique', 'historique', 'historique', 'historique', 'romance', 'historique', 'romance', 'historique', 'horreur', 'historique', 'historique', 'historique', 'historique', 'romance', 'drame', 'historique', 'drame', 'drame', 'romance', 'historique', 'policier', 'policier', 'documentaire', 'com√©die', 'drame', 'historique', 'drame', 'historique', 'drame', 'romance', 'policier', 'policier', 'historique', 'historique', 'drame', 'historique', 'horreur', 'drame', 'drame', 'drame', 'historique', 'policier', 'policier', 'historique', 'horreur', 'historique', 'drame', 'horreur', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'policier', 'documentaire', 'drame', 'policier', 'policier', 'horreur', 'romance', 'com√©die', 'policier', 'historique', 'historique', 'policier', 'com√©die', 'romance', 'com√©die', 'historique', 'historique', 'com√©die', 'documentaire', 'horreur', 'historique', 'drame', 'drame', 'drame', 'documentaire', 'historique', 'historique', 'horreur', 'historique', 'documentaire', 'drame', 'drame', 'historique', 'drame', 'policier', 'biopic', 'historique', 'historique', 'com√©die', 'drame', 'historique', 'romance', 'policier', 'policier', 'drame', 'historique', 'policier', 'policier', 'historique', 'documentaire', 'drame', 'historique', 'com√©die', 'drame', 'documentaire', 'romance', 'policier', 'science fiction', 'policier', 'science fiction', 'drame', 'drame', 'historique', 'drame', 'policier', 'com√©die', 'drame', 'historique', 'policier', 'policier', 'romance', 'historique', 'historique', 'historique', 'documentaire', 'historique', 'policier', 'historique', 'policier', 'documentaire', 'drame', 'drame', 'horreur', 'horreur', 'romance', 'historique', 'historique', 'romance', 'historique', 'romance', 'horreur', 'horreur', 'policier', 'horreur', 'policier', 'drame', 'documentaire', 'drame', 'historique', 'historique', 'science fiction', 'drame', 'policier', 'historique', 'historique', 'policier', 'drame', 'drame', 'historique', 'policier', 'historique', 'drame', 'horreur', 'documentaire', 'policier', 'drame', 'drame', 'historique', 'historique', 'horreur', 'policier', 'historique', 'historique', 'historique', 'historique', 'policier', 'drame', 'policier', 'historique', 'documentaire', 'romance', 'drame', 'historique', 'science fiction', 'com√©die', 'historique', 'documentaire', 'horreur', 'historique', 'historique', 'drame', 'historique', 'drame', 'historique', 'policier', 'documentaire', 'historique', 'documentaire', 'horreur', 'drame', 'historique', 'historique', 'documentaire', 'historique', 'horreur', 'policier', 'historique', 'historique', 'policier', 'romance', 'com√©die', 'drame', 'documentaire', 'historique', 'documentaire', 'policier', 'historique', 'policier', 'policier', 'drame', 'policier', 'drame', 'historique', 'horreur', 'policier', 'documentaire', 'historique', 'historique', 'drame', 'drame', 'historique', 'historique', 'com√©die', 'historique', 'historique', 'com√©die', 'drame', 'historique', 'com√©die', 'historique', 'policier', 'com√©die', 'documentaire', 'drame', 'historique', 'historique', 'historique', 'documentaire', 'documentaire', 'documentaire', 'horreur', 'documentaire', 'documentaire', 'historique', 'drame', 'historique', 'documentaire', 'historique', 'drame', 'documentaire', 'historique', 'romance', 'policier', 'historique', 'horreur', 'historique', 'science fiction', 'science fiction', 'historique', 'historique', 'romance', 'documentaire', 'policier', 'historique', 'drame', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'drame', 'romance', 'drame', 'drame', 'drame', 'policier', 'drame', 'documentaire', 'documentaire', 'drame', 'historique', 'drame', 'documentaire', 'policier', 'historique', 'drame', 'policier', 'historique', 'drame', 'romance', 'horreur', 'documentaire', 'historique', 'historique', 'horreur', 'horreur', 'policier', 'drame', 'romance', 'historique', 'historique', 'drame', 'historique', 'historique', 'historique', 'drame', 'horreur', 'drame', 'historique', 'drame', 'drame', 'romance', 'historique', 'policier', 'historique', 'historique', 'policier', 'drame', 'historique', 'horreur', 'historique', 'horreur', 'documentaire', 'documentaire', 'drame', 'historique', 'historique', 'drame', 'romance', 'documentaire', 'science fiction', 'historique', 'drame', 'documentaire', 'historique', 'historique', 'drame', 'romance', 'historique', 'documentaire', 'horreur', 'policier', 'policier', 'horreur', 'historique', 'drame', 'drame', 'com√©die', 'drame', 'drame', 'drame', 'drame', 'policier', 'historique', 'policier', 'documentaire', 'documentaire', 'historique', 'romance', 'historique', 'horreur', 'historique', 'drame', 'drame', 'historique', 'policier', 'drame', 'policier', 'documentaire', 'policier', 'com√©die', 'documentaire', 'drame', 'policier', 'romance', 'historique', 'drame', 'historique', 'drame', 'com√©die', 'drame', 'drame', 'drame', 'romance', 'drame', 'romance', 'drame', 'drame', 'romance', 'romance', 'documentaire', 'horreur', 'documentaire', 'drame', 'historique', 'historique', 'historique', 'historique', 'historique', 'documentaire', 'policier', 'historique', 'policier', 'historique', 'historique', 'drame', 'drame', 'drame', 'drame', 'policier', 'historique', 'historique', 'com√©die', 'historique', 'horreur', 'romance', 'historique', 'drame', 'policier', 'policier', 'drame', 'policier', 'drame', 'policier', 'horreur', 'historique', 'drame', 'drame', 'drame', 'documentaire', 'historique', 'historique', 'com√©die', 'historique', 'policier', 'historique', 'documentaire', 'documentaire', 'drame', 'romance', 'drame', 'drame', 'com√©die', 'drame', 'documentaire', 'drame', 'romance', 'drame', 'documentaire', 'policier', 'historique', 'historique', 'policier', 'drame', 'historique', 'horreur', 'drame', 'historique', 'historique', 'historique', 'historique', 'drame', 'historique', 'drame', 'historique', 'historique', 'policier', 'documentaire', 'horreur', 'drame', 'romance', 'documentaire', 'policier', 'historique', 'drame', 'drame', 'historique', 'documentaire', 'com√©die', 'policier', 'com√©die', 'historique', 'drame', 'historique', 'com√©die', 'policier', 'drame', 'com√©die', 'historique', 'drame', 'documentaire', 'horreur', 'com√©die', 'drame', 'drame', 'science fiction', 'documentaire', 'historique', 'historique', 'drame', 'romance', 'com√©die', 'historique', 'historique', 'drame', 'drame', 'romance', 'policier', 'policier', 'drame', 'historique', 'policier', 'documentaire', 'horreur', 'romance', 'romance', 'drame', 'policier', 'drame', 'drame', 'policier', 'drame', 'historique', 'policier', 'historique', 'historique', 'documentaire', 'policier', 'historique', 'romance', 'historique', 'documentaire', 'historique', 'drame', 'documentaire', 'documentaire', 'documentaire', 'historique', 'drame', 'drame', 'historique', 'documentaire', 'documentaire', 'documentaire', 'historique', 'documentaire', 'drame', 'policier', 'documentaire', 'drame', 'drame', 'documentaire', 'drame', 'historique', 'drame', 'historique', 'policier', 'com√©die', 'drame', 'historique', 'romance', 'historique', 'historique', 'romance', 'historique', 'historique', 'documentaire', 'drame', 'drame', 'drame', 'policier', 'drame', 'drame', 'drame', 'historique', 'drame', 'documentaire', 'historique', 'com√©die', 'drame', 'historique', 'drame', 'com√©die', 'romance', 'historique', 'historique', 'historique', 'policier', 'drame', 'documentaire', 'drame', 'historique', 'com√©die', 'historique', 'romance', 'drame', 'horreur', 'com√©die', 'historique', 'drame', 'documentaire', 'horreur', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'documentaire', 'historique', 'historique', 'policier', 'drame', 'policier', 'policier', 'documentaire', 'policier', 'com√©die', 'documentaire', 'romance', 'com√©die', 'drame', 'historique', 'documentaire', 'documentaire', 'policier', 'drame', 'historique', 'policier', 'policier', 'historique', 'historique', 'historique', 'com√©die', 'historique', 'documentaire', 'historique', 'romance', 'policier', 'historique', 'historique', 'com√©die', 'science fiction', 'historique', 'romance', 'romance', 'drame', 'historique', 'drame', 'historique', 'science fiction', 'historique', 'biopic', 'drame', 'documentaire', 'documentaire', 'historique', 'historique', 'historique', 'drame', 'policier', 'science fiction', 'documentaire', 'biopic', 'historique', 'drame', 'drame', 'drame', 'romance', 'historique', 'com√©die', 'drame', 'biopic', 'horreur', 'historique', 'historique']\n",
            "['romance', 'science fiction', 'romance', 'biopic', 'science fiction', 'horreur', 'romance', 'drame', 'romance', 'romance', 'historique', 'policier', 'science fiction', 'biopic', 'documentaire', 'drame', 'com√©die', 'historique', 'horreur', 'drame', 'science fiction', 'drame', 'horreur', 'science fiction', 'com√©die', 'drame', 'drame', 'policier', 'science fiction', 'biopic', 'romance', 'com√©die', 'com√©die', 'drame', 'drame', 'documentaire', 'documentaire', 'biopic', 'drame', 'historique', 'drame', 'romance', 'horreur', 'biopic', 'drame', 'biopic', 'romance', 'com√©die', 'policier', 'policier', 'policier', 'com√©die', 'drame', 'historique', 'horreur', 'biopic', 'com√©die', 'romance', 'policier', 'policier', 'science fiction', 'documentaire', 'romance', 'historique', 'horreur', 'science fiction', 'drame', 'drame', 'drame', 'policier', 'policier', 'historique', 'policier', 'policier', 'horreur', 'horreur', 'romance', 'historique', 'com√©die', 'romance', 'policier', 'horreur', 'policier', 'documentaire', 'com√©die', 'policier', 'science fiction', 'horreur', 'com√©die', 'com√©die', 'historique', 'drame', 'romance', 'biopic', 'historique', 'drame', 'romance', 'romance', 'historique', 'drame', 'horreur', 'science fiction', 'romance', 'com√©die', 'drame', 'policier', 'com√©die', 'drame', 'drame', 'horreur', 'documentaire', 'horreur', 'romance', 'com√©die', 'romance', 'science fiction', 'science fiction', 'com√©die', 'drame', 'drame', 'com√©die', 'com√©die', 'romance', 'drame', 'policier', 'biopic', 'drame', 'drame', 'com√©die', 'policier', 'horreur', 'com√©die', 'drame', 'romance', 'drame', 'science fiction', 'documentaire', 'romance', 'drame', 'science fiction', 'policier', 'horreur', 'com√©die', 'science fiction', 'science fiction', 'com√©die', 'horreur', 'com√©die', 'romance', 'biopic', 'romance', 'policier', 'drame', 'documentaire', 'com√©die', 'historique', 'romance', 'drame', 'drame', 'com√©die', 'romance', 'romance', 'science fiction', 'com√©die', 'horreur', 'horreur', 'romance', 'drame', 'horreur', 'romance', 'drame', 'romance', 'horreur', 'horreur', 'policier', 'horreur', 'policier', 'policier', 'drame', 'science fiction', 'drame', 'biopic', 'science fiction', 'science fiction', 'policier', 'drame', 'com√©die', 'romance', 'com√©die', 'drame', 'historique', 'policier', 'science fiction', 'com√©die', 'science fiction', 'science fiction', 'science fiction', 'drame', 'drame', 'romance', 'romance', 'horreur', 'drame', 'drame', 'drame', 'romance', 'historique', 'policier', 'com√©die', 'policier', 'romance', 'romance', 'romance', 'romance', 'science fiction', 'science fiction', 'romance', 'drame', 'horreur', 'horreur', 'historique', 'horreur', 'policier', 'science fiction', 'science fiction', 'horreur', 'horreur', 'policier', 'biopic', 'drame', 'science fiction', 'science fiction', 'historique', 'drame', 'documentaire', 'horreur', 'romance', 'policier', 'science fiction', 'com√©die', 'com√©die', 'romance', 'romance', 'documentaire', 'com√©die', 'biopic', 'historique', 'drame', 'horreur', 'policier', 'policier', 'horreur', 'horreur', 'horreur', 'romance', 'horreur', 'policier', 'science fiction', 'com√©die', 'historique', 'com√©die', 'drame', 'romance', 'horreur', 'romance', 'historique', 'biopic', 'com√©die', 'com√©die', 'science fiction', 'horreur', 'horreur', 'policier', 'horreur', 'drame', 'policier', 'science fiction', 'documentaire', 'science fiction', 'drame', 'biopic', 'documentaire', 'policier', 'documentaire', 'com√©die', 'historique', 'horreur', 'documentaire', 'com√©die', 'science fiction', 'com√©die', 'horreur', 'horreur', 'com√©die', 'biopic', 'documentaire', 'horreur', 'drame', 'com√©die', 'horreur', 'com√©die', 'com√©die', 'romance', 'drame', 'policier', 'drame', 'com√©die', 'drame', 'drame', 'drame', 'drame', 'com√©die', 'historique', 'romance', 'drame', 'horreur', 'romance', 'com√©die', 'policier', 'science fiction', 'com√©die', 'science fiction', 'biopic', 'biopic', 'science fiction', 'documentaire', 'policier', 'historique', 'com√©die', 'policier', 'com√©die', 'policier', 'romance', 'policier', 'drame', 'horreur', 'drame', 'horreur', 'horreur', 'drame', 'horreur', 'romance', 'drame', 'historique', 'drame', 'romance', 'historique', 'science fiction', 'policier', 'science fiction', 'com√©die', 'biopic', 'drame', 'science fiction', 'romance', 'com√©die', 'science fiction', 'drame', 'documentaire', 'policier', 'horreur', 'drame', 'policier', 'science fiction', 'science fiction', 'biopic', 'historique', 'romance', 'documentaire', 'historique', 'horreur', 'romance', 'romance', 'com√©die', 'biopic', 'romance', 'drame', 'horreur', 'biopic', 'drame', 'romance', 'historique', 'com√©die', 'historique', 'biopic', 'policier', 'horreur', 'historique', 'romance', 'policier', 'com√©die', 'science fiction', 'drame', 'romance', 'com√©die', 'policier', 'biopic', 'horreur', 'biopic', 'com√©die', 'horreur', 'com√©die', 'historique', 'horreur', 'biopic', 'drame', 'romance', 'horreur', 'com√©die', 'horreur', 'policier', 'drame', 'policier', 'romance', 'documentaire', 'horreur', 'drame', 'romance', 'romance', 'com√©die', 'historique', 'science fiction', 'romance', 'com√©die', 'drame', 'science fiction', 'romance', 'drame', 'com√©die', 'romance', 'romance', 'com√©die', 'drame', 'drame', 'romance', 'documentaire', 'documentaire', 'documentaire', 'science fiction', 'romance', 'drame', 'biopic', 'historique', 'com√©die', 'drame', 'com√©die', 'romance', 'science fiction', 'drame', 'science fiction', 'horreur', 'horreur', 'policier', 'drame', 'biopic', 'com√©die', 'romance', 'horreur', 'drame', 'historique', 'drame', 'policier', 'drame', 'com√©die', 'policier', 'com√©die', 'horreur', 'com√©die', 'romance', 'policier', 'biopic', 'romance', 'historique', 'romance', 'historique', 'biopic', 'biopic', 'policier', 'science fiction', 'historique', 'com√©die', 'science fiction', 'drame', 'romance', 'science fiction', 'com√©die', 'romance', 'policier', 'documentaire', 'romance', 'romance', 'documentaire', 'horreur', 'science fiction', 'horreur', 'policier', 'drame', 'historique', 'horreur', 'com√©die', 'drame', 'biopic', 'science fiction', 'biopic', 'drame', 'horreur', 'drame', 'drame', 'biopic', 'romance', 'documentaire', 'horreur', 'biopic', 'romance', 'documentaire', 'policier', 'com√©die', 'romance', 'com√©die', 'historique', 'drame', 'com√©die', 'science fiction', 'drame', 'com√©die', 'science fiction', 'romance', 'romance', 'policier', 'com√©die', 'com√©die', 'policier', 'drame', 'documentaire', 'horreur', 'com√©die', 'horreur', 'horreur', 'horreur', 'romance', 'drame', 'com√©die', 'historique', 'romance', 'romance', 'biopic', 'science fiction', 'drame', 'science fiction', 'romance', 'com√©die', 'policier', 'com√©die', 'policier', 'drame', 'documentaire', 'horreur', 'com√©die', 'com√©die', 'science fiction', 'com√©die', 'drame', 'com√©die', 'science fiction', 'horreur', 'historique', 'policier', 'science fiction', 'science fiction', 'com√©die', 'historique', 'science fiction', 'biopic', 'science fiction', 'com√©die', 'science fiction', 'horreur', 'biopic', 'romance', 'romance', 'science fiction', 'com√©die', 'biopic', 'drame', 'drame', 'documentaire', 'com√©die', 'documentaire', 'drame', 'com√©die', 'drame', 'com√©die', 'com√©die', 'romance', 'biopic', 'com√©die', 'com√©die', 'science fiction', 'com√©die', 'policier', 'drame', 'horreur', 'historique', 'policier', 'romance', 'drame', 'com√©die', 'drame', 'biopic', 'horreur', 'com√©die', 'horreur', 'drame', 'policier', 'policier', 'com√©die', 'com√©die', 'science fiction', 'com√©die', 'romance', 'science fiction', 'drame', 'historique', 'science fiction', 'documentaire', 'com√©die', 'com√©die', 'romance', 'drame', 'historique', 'policier', 'science fiction', 'biopic', 'romance', 'com√©die', 'com√©die', 'drame', 'drame', 'romance', 'com√©die', 'romance', 'science fiction', 'romance', 'documentaire', 'horreur', 'science fiction', 'biopic', 'science fiction', 'drame', 'documentaire', 'drame', 'horreur', 'biopic', 'romance', 'policier', 'documentaire', 'policier', 'science fiction', 'romance', 'policier', 'com√©die', 'horreur', 'com√©die', 'romance', 'romance', 'drame', 'drame', 'horreur', 'com√©die', 'policier', 'horreur', 'policier', 'policier', 'historique', 'science fiction', 'policier', 'com√©die', 'drame', 'com√©die', 'historique', 'romance', 'policier', 'science fiction', 'science fiction', 'romance', 'documentaire', 'horreur', 'com√©die', 'com√©die', 'historique', 'drame', 'drame', 'historique', 'com√©die', 'romance', 'historique', 'science fiction', 'romance', 'romance', 'historique', 'historique', 'com√©die', 'science fiction', 'drame', 'science fiction', 'documentaire', 'com√©die', 'romance', 'drame', 'documentaire', 'romance', 'romance', 'biopic', 'romance', 'com√©die', 'biopic', 'horreur', 'biopic', 'romance']\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         biopic       0.25      0.02      0.04        47\n",
            "        com√©die       0.41      0.14      0.21       116\n",
            "   documentaire       0.20      0.43      0.27        37\n",
            "          drame       0.19      0.28      0.23       117\n",
            "     historique       0.16      0.72      0.26        50\n",
            "        horreur       0.62      0.30      0.40        84\n",
            "       policier       0.53      0.68      0.59        75\n",
            "        romance       0.50      0.22      0.30       110\n",
            "science fiction       0.36      0.05      0.09        83\n",
            "\n",
            "       accuracy                           0.29       719\n",
            "      macro avg       0.36      0.32      0.27       719\n",
            "   weighted avg       0.38      0.29      0.27       719\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(X_test)\n",
        "print(y_pred)\n",
        "print(y_test.to_list())\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
