{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projet : Traitement Automatique des Langues (Partie 1)\n",
        "\n",
        "Nous avons à notre disposition deux fichiers CSV ([allocine_genres_test.csv](data/allocine_genres_test.csv) et [allocine_genres_train.csv](data/allocine_genres_train.csv)) contenant des informations sur des films et leurs genres. Le but de ce projet est de prédire les genres d'un film à partir de son synopsis notamment (et d'autres informations).\n",
        "\n",
        "L’objectif est d’entraîner un outil de classification automatique des films en fonction de leur genre. La classification doit se baser sur le texte de la synopsis et sur le titre des films. Le texte et le titre des articles ont déjà été tokenisés et tous les tokens sont séparés par un espace."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importation des données et analyse exploratoire\n",
        "\n",
        "Les données sont disponibles dans le dossier [data](data/). Nous allons commencer par importer les données et les analyser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# exécuter cette cellule pour installer les dépendances et télécharger les modèles spacy\n",
        "# remplacer `python` par `python3` si nécessaire\n",
        "# !conda create -n nlp python=3.11\n",
        "# !conda activate nlp\n",
        "!python -m pip install --upgrade -r requirements.txt\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import FrenchStemmer\n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_train.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 22)\n",
            "   Unnamed: 0         acteur_1           acteur_2           acteur_3   \n",
            "0        4772    Albert Finney      Lauren Bacall  Jacqueline Bisset  \\\n",
            "1         335      Henry Fonda      Martin Balsam       John Fiedler   \n",
            "2        4860   Alexandra Lamy  Michaël Abiteboul       Julia Piaton   \n",
            "3        1913  Charles Chaplin  Virginia Cherrill        Harry Myers   \n",
            "4        3726   Robert De Niro   Cuba Gooding Jr.    Charlize Theron   \n",
            "\n",
            "   allocine_id  annee_prod  annee_sortie  box_office_fr  couleur  duree  ...   \n",
            "0         1453        1974        1975.0       549055.0  Couleur  128.0  ...  \\\n",
            "1         4063        1957        1957.0            NaN      NaN   95.0  ...   \n",
            "2       241952        2016        2019.0            NaN  Couleur   90.0  ...   \n",
            "3         2256        1931        1931.0            NaN      NaN   87.0  ...   \n",
            "4        27434        2000        2001.0       221677.0  Couleur  129.0  ...   \n",
            "\n",
            "  nb_critiques_presse nb_critiques_spectateurs  nb_notes_spectateurs   \n",
            "0                 NaN                    125.0                2045.0  \\\n",
            "1                 7.0                    771.0               18670.0   \n",
            "2                 NaN                     22.0                 242.0   \n",
            "3                 4.0                    190.0                6185.0   \n",
            "4                17.0                    104.0                1619.0   \n",
            "\n",
            "   note_presse  note_spectateurs        realisateurs   \n",
            "0          NaN               3.7        Sidney Lumet  \\\n",
            "1          5.0               4.6        Sidney Lumet   \n",
            "2          NaN               4.2       Nicolas Cuche   \n",
            "3          4.5               4.4     Charles Chaplin   \n",
            "4          2.4               3.6  George Tillman Jr.   \n",
            "\n",
            "                                            synopsis   \n",
            "0  En visite à Istanbul , le célèbre détective be...  \\\n",
            "1  Un jeune homme d' origine modeste est accusé d...   \n",
            "2  Lorsque Marie-Laure , mère de quatre jeunes en...   \n",
            "3  Un vagabond s’ éprend d’ une belle et jeune ve...   \n",
            "4  L' histoire vraie de Carl Brashear , premier A...   \n",
            "\n",
            "                             titre     type_film     genre  \n",
            "0  Le Crime de l' Orient - Express           NaN  policier  \n",
            "1              12 hommes en colère           NaN     drame  \n",
            "2             Après moi le bonheur      Télefilm     drame  \n",
            "3         Les Lumières de la ville           NaN   romance  \n",
            "4        Les Chemins de la dignité  Long-métrage    biopic  \n",
            "\n",
            "[5 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "genre\n",
            "drame              501\n",
            "comédie            483\n",
            "romance            443\n",
            "policier           331\n",
            "horreur            299\n",
            "science fiction    298\n",
            "biopic             191\n",
            "documentaire       167\n",
            "historique         162\n",
            "Name: count, dtype: int64\n",
            "\n",
            "La prodigieuse histoire vraie d’ une jeune femme surdouée devenue la reine d’ un gigantesque empire du jeu clandestin à Hollywood ! En 2004 , la jeune Molly Bloom débarque à Los Angeles . Simple assistante , elle épaule son patron qui réunit toutes les semaines des joueurs de poker autour de parties clandestines . Virée sans ménagement , elle décide de monter son propre cercle : la mise d’ entrée sera de 250 000 $ ! Très vite , les stars hollywoodiennes , les millionnaires et les grands sportifs accourent . Le succès est immédiat et vertigineux . Acculée par les agents du FBI décidés à la faire tomber , menacée par la mafia russe décidée à faire main basse sur son activité , et harcelée par des célébrités inquiètes qu’ elle ne les trahisse , Molly Bloom se retrouve prise entre tous les feux …\n"
          ]
        }
      ],
      "source": [
        "print(train['genre'].value_counts(), end='\\n\\n')\n",
        "print(random.choice(train['synopsis'].unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque déjà que les donnés chiffrées sont soit des entiers soit des flottants. Les données textuelles sont des chaînes de caractères. Certaines données sont manquantes : `NaN` dans le cas des données chiffrées et une chaîne vide dans le cas des données textuelles.\n",
        "\n",
        "Pour traiter les données manquantes, nous avons deux solutions :\n",
        "\n",
        "1. Dans le cas où il y a très peu de données manquantes, on peut simplement supprimer les entrées qui contiennent ces données manquantes. Cela peut être acceptable si le nombre de données manquantes est très faible par rapport à la taille de l'ensemble de données et que la suppression de ces entrées n'affecte pas significativement les résultats de l'analyse.\n",
        "2. En revanche, si le nombre de données manquantes est important, la suppression de ces entrées pourrait entraîner une perte d'informations importantes pour l'analyse. Dans ce cas, il est généralement préférable de remplacer les valeurs manquantes par une valeur qui représente au mieux l'information manquante. Par exemple, si les données manquantes sont des scores au box-office, vous pouvez remplacer ces données manquantes par la moyenne ou la médiane des scores de box-office disponibles dans les données.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(891, 22)\n",
            "(2875, 22)\n"
          ]
        }
      ],
      "source": [
        "EMPTY_TOKEN = '<EMPTY>'\n",
        "\n",
        "# remove rows with missing values\n",
        "train_dataset_1 = train.dropna(how='any', inplace=False)\n",
        "\n",
        "# replace missing values with either the mean or the median (or empty token)\n",
        "list_headers = train.columns.values.tolist()\n",
        "list_of_numerical_headers = train._get_numeric_data().columns.values.tolist()\n",
        "list_of_categorical_headers = list(set(list_headers) - set(list_of_numerical_headers))\n",
        "\n",
        "train_dataset_2 = train.copy()\n",
        "for header in list_of_numerical_headers:\n",
        "  train_dataset_2[header].fillna(train_dataset_2[header].median(), inplace=True)\n",
        "for header in list_of_categorical_headers:\n",
        "  train_dataset_2[header].fillna(EMPTY_TOKEN, inplace=True)\n",
        "\n",
        "print(train_dataset_1.shape)\n",
        "print(train_dataset_2.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lorsqu'on supprime simplement toutes les entrées où il manque au moins une valeur, on se retrouve uniquement avec 891 valeurs en tout. Cela signifie que nous avons perdu beaucoup d'informations. Nous allons donc utiliser la deuxième solution (au moins dans un premier temps) et remplacer les valeurs manquantes par des valeurs qui représentent au mieux l'information manquante."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prétraitement des données\n",
        "\n",
        "Il faut aussi corriger les entrées textuelles, ainsi qu'appliquer un certain nombre d'algorithmes de prétraitement comme : la suppression des caractères spéciaux, la suppression des stop words, la suppression des mots trop fréquents ou trop rares, la lemmatisation, la suppression des mots trop longs, etc.\n",
        "\n",
        "On définit donc un ensemble de fonctions et de filtres qui vont nous permettre de prétraiter les données textuelles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)        # download the stopwords corpus\n",
        "nlp = spacy.load('fr_core_news_sm')           # load the French model\n",
        "fr_stopwords = set(stopwords.words('french')) # so that `in` tests are faster\n",
        "stemmer = FrenchStemmer()                     # for stemming words\n",
        "\n",
        "\n",
        "# get the tokens of a sentence (word based tokenization)\n",
        "def get_tokens_words(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [w.text for w in doc]\n",
        "\n",
        "\n",
        "# remove stopwords from a sentence\n",
        "def clean_sentence(text: str) -> list[str]:\n",
        "  clean_words: list[str] = []\n",
        "  for token in get_tokens_words(text):\n",
        "    if token not in fr_stopwords:\n",
        "      clean_words.append(token)\n",
        "  return clean_words\n",
        "\n",
        "\n",
        "# get the tokens of multiple sentences (sentence based tokenization)\n",
        "def get_tokens_sentences(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [s.text for s in doc.sents]\n",
        "\n",
        "\n",
        "# get the lemmas of a sentence\n",
        "def get_stem(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [stemmer.stem(w.text) for w in doc]\n",
        "\n",
        "\n",
        "# get the named entities of a sentence\n",
        "def get_ner(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "\n",
        "# render the named entities of a sentence in a Jupyter notebook\n",
        "def render_ner(text: str) -> None:\n",
        "  doc = nlp(text)\n",
        "  displacy.render(doc, style='ent', jupyter=True)\n",
        "\n",
        "\n",
        "# get the part of speech of a sentence\n",
        "def get_pos(text: str) -> list[str]:\n",
        "  doc = nlp(text)\n",
        "  return [(token, token.pos_) for token in doc]\n",
        "\n",
        "\n",
        "# render the part of speech of a sentence in a Jupyter notebook\n",
        "def render_pos(text: str) -> None:\n",
        "  doc = nlp(text)\n",
        "  displacy.render(doc, style='dep', options={'distance': 90})\n",
        "\n",
        "\n",
        "# get the word embeddings of a sentence\n",
        "def get_word_embeddings(text: str) -> list[np.ndarray]:\n",
        "  doc = nlp(text)\n",
        "  return [token.vector for token in doc]\n",
        "\n",
        "\n",
        "# get the similarity between two sentences\n",
        "def get_mean_embedding(text1: str, text2: str) -> float:\n",
        "  doc1 = nlp(text1)\n",
        "  doc2 = nlp(text2)\n",
        "  mean1 = np.mean([token.vector for token in doc1], axis=0)\n",
        "  mean2 = np.mean([token.vector for token in doc2], axis=0)\n",
        "\n",
        "  return np.dot(mean1, mean2) / (np.linalg.norm(mean1) * np.linalg.norm(mean2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Juste pour clarifier les choses, nous allons simplement effectuer des tests sur l'ensemble de phrases suivantes :\n",
        "\n",
        "1. \"Le réseau sera bientôt rétabli à Marseille\"\n",
        "2. \"La panne réseau affecte plusieurs utilisateurs de l'opérateur\"\n",
        "3. \"Il fait 18 degrés ici\"\n",
        "4. \"Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "text1 = 'Le réseau sera bientôt rétabli à Marseille.'\n",
        "text2 = 'La panne réseau affecte plusieurs utilisateurs de l\\'opérateur'\n",
        "text3 = 'Il fait 18 degrés ici'\n",
        "text4 = 'Bouygues a eu une coupure de réseau à Marseille. La panne a affecté 300.000 utilisateurs.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Le', 'réseau', 'sera', 'bientôt', 'rétabli', 'à', 'Marseille', '.']\n",
            "['Le', 'réseau', 'bientôt', 'rétabli', 'Marseille', '.']\n",
            "['Bouygues a eu une coupure de réseau à Marseille.', 'La panne a affecté 300.000 utilisateurs.']\n"
          ]
        }
      ],
      "source": [
        "# basic tokenization\n",
        "# we can observe `get_tokens_sentences` do not \"cut\" at each . or ! or ?\n",
        "\n",
        "print(get_tokens_words(text1))\n",
        "print(clean_sentence(text1))\n",
        "print(get_tokens_sentences(text4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['le', 'réseau', 'ser', 'bientôt', 'rétabl', 'à', 'marseil', '.']\n",
            "['la', 'pann', 'réseau', 'affect', 'plusieur', 'utilis', 'de', \"l'\", 'oper']\n"
          ]
        }
      ],
      "source": [
        "# stemming\n",
        "# this doesn't work very well for French...\n",
        "\n",
        "print(get_stem(text1))\n",
        "print(get_stem(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Bouygues', 'ORG'), ('Marseille', 'LOC')]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bouygues\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " a eu une coupure de réseau à \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Marseille\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". La panne a affecté 300.000 utilisateurs.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# named entities recognition\n",
        "\n",
        "print(get_ner(text4))\n",
        "render_ner(text4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Le, 'DET'), (réseau, 'NOUN'), (sera, 'AUX'), (bientôt, 'ADV'), (rétabli, 'ADJ'), (à, 'ADP'), (Marseille, 'PROPN'), (., 'PUNCT')]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"8161221763a9498dab35ad80b9fe3d91-0\" class=\"displacy\" width=\"680\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Le</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">réseau</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">sera</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">bientôt</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">rétabli</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">à</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">Marseille.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,2.0 410.0,2.0 410.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,92.0 580.0,92.0 580.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8161221763a9498dab35ad80b9fe3d91-0-5\" stroke-width=\"2px\" d=\"M430,137.0 C430,47.0 585.0,47.0 585.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8161221763a9498dab35ad80b9fe3d91-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:arg</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M585.0,139.0 L593.0,127.0 577.0,127.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# part of speech\n",
        "\n",
        "print(get_pos(text1))\n",
        "render_pos(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(96,)\n",
            "0.29973912\n",
            "0.39468837\n",
            "0.53404236\n"
          ]
        }
      ],
      "source": [
        "# word embeddings and mean embedding (similarity)\n",
        "\n",
        "print(get_word_embeddings(text1)[0].shape)\n",
        "print(get_mean_embedding(text1, text2))\n",
        "print(get_mean_embedding(text1, text4))\n",
        "print(get_mean_embedding(text2, text4))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Préparation des données\n",
        "\n",
        "Nous allons maintenant préparer les données pour l'entraînement de notre modèle.\n",
        "\n",
        "Nous allons donc appliquer les fonctions de prétraitement sur les données textuelles et transformer les données chiffrées en données numériques. Dans un premier temps, nous confectionnerons des ensembles contenant toutes les valeurs et les informations présentes dans les données. Nous verrons par la suites lesquelles sont les plus pertinantes en fonction des résultats obtenus et de nos modèles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 2)\n",
            "                                            synopsis   \n",
            "0  En visite à Istanbul , le célèbre détective be...  \\\n",
            "1  Un jeune homme d' origine modeste est accusé d...   \n",
            "2  Lorsque Marie-Laure , mère de quatre jeunes en...   \n",
            "3  Un vagabond s’ éprend d’ une belle et jeune ve...   \n",
            "4  L' histoire vraie de Carl Brashear , premier A...   \n",
            "\n",
            "                             titre  \n",
            "0  Le Crime de l' Orient - Express  \n",
            "1              12 hommes en colère  \n",
            "2             Après moi le bonheur  \n",
            "3         Les Lumières de la ville  \n",
            "4        Les Chemins de la dignité  \n",
            "(2875,)\n",
            "0    policier\n",
            "1       drame\n",
            "2       drame\n",
            "3     romance\n",
            "4      biopic\n",
            "Name: genre, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# we will use train_dataset_2 since it has no missing values\n",
        "\n",
        "values = ['synopsis', 'titre']\n",
        "X = train_dataset_2[values]\n",
        "y = train_dataset_2['genre']\n",
        "\n",
        "print(X.shape)\n",
        "print(X.head())\n",
        "print(y.shape)\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separate the dataset into a training set and a validation set\n",
        "X_train, y_train = X, y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour la suite, nous définirons des pipelines de traitement spécifiques à chaque type de colonne. En particulier, les colonnes correspondant à des textes (`list_of_categorical_headers`) dans `X` seront vectorisées.\n",
        "\n",
        "Pour toutes les données dans `list_of_categorical_headers`, les tokens ont déjà été séparés par des espaces. Nous allons utiliser `TfidfVectorizer` pour vectoriser ces données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '), # because the text is already tokenized\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  min_df=0.01,\n",
        "  max_df=0.95,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 678)\n",
            "     !    \"    (    )         -       ...   20   30         :    ;  ...  état   \n",
            "0  0.0  0.0  0.0  0.0  0.117280  0.000000  0.0  0.0  0.208365  0.0  ...   0.0  \\\n",
            "1  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.122109  0.0  ...   0.0   \n",
            "2  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.0  ...   0.0   \n",
            "3  0.0  0.0  0.0  0.0  0.000000  0.197675  0.0  0.0  0.000000  0.0  ...   0.0   \n",
            "4  0.0  0.0  0.0  0.0  0.141757  0.000000  0.0  0.0  0.000000  0.0  ...   0.0   \n",
            "\n",
            "   étrange  étranges  étudiant  événements      être  êtres  île    –   \n",
            "0      0.0       0.0       0.0         0.0  0.000000    0.0  0.0  0.0  \\\n",
            "1      0.0       0.0       0.0         0.0  0.157641    0.0  0.0  0.0   \n",
            "2      0.0       0.0       0.0         0.0  0.000000    0.0  0.0  0.0   \n",
            "3      0.0       0.0       0.0         0.0  0.000000    0.0  0.0  0.0   \n",
            "4      0.0       0.0       0.0         0.0  0.000000    0.0  0.0  0.0   \n",
            "\n",
            "          …  \n",
            "0  0.116678  \n",
            "1  0.000000  \n",
            "2  0.000000  \n",
            "3  0.000000  \n",
            "4  0.000000  \n",
            "\n",
            "[5 rows x 678 columns]\n"
          ]
        }
      ],
      "source": [
        "res = tfidf_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous pouvons aussi utiliser ce `vectorizer` pour extraire des statistiques sur les données textuelles. Par exemple, la longueur en nombre de caractères, le nombre de phrases, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_stats(texts: list[str]) -> list[dict[str, int]]:\n",
        "  return [{\n",
        "    'len': len(t),\n",
        "    'nb_sentences': t.count('.') + t.count('!') + t.count('?'),\n",
        "  } for t in texts]\n",
        "\n",
        "stats_transformer = FunctionTransformer(make_stats, validate=False)\n",
        "stats_vectorizer = DictVectorizer(sparse=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 2)\n",
            "     len  nb_sentences\n",
            "0  677.0           4.0\n",
            "1  460.0           4.0\n",
            "2  560.0           4.0\n",
            "3  233.0           4.0\n",
            "4  340.0           2.0\n"
          ]
        }
      ],
      "source": [
        "res = stats_vectorizer.fit_transform(stats_transformer.transform(X_train['synopsis']))\n",
        "stats = pd.DataFrame(res, columns=stats_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(stats.shape)\n",
        "print(stats.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On normalise les données en utilisant `MinMaxScaler` pour les données dans notre dictionnaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 2)\n",
            "        len  nb_sentences\n",
            "0  0.340062      0.129032\n",
            "1  0.227743      0.129032\n",
            "2  0.279503      0.129032\n",
            "3  0.110248      0.129032\n",
            "4  0.165631      0.064516\n"
          ]
        }
      ],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "scaled_stats = pd.DataFrame(min_max_scaler.fit_transform(stats), columns=stats.columns)\n",
        "\n",
        "print(scaled_stats.shape)\n",
        "print(scaled_stats.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On rajoute aussi `CountVectorizer` ainsi que `HashingVectorizer` pour voir si les résultats sont meilleurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_vectorizer = CountVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '),\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  min_df=0.01,\n",
        "  max_df=0.95,\n",
        ")\n",
        "\n",
        "# not used here\n",
        "hashing_vectorizer = HashingVectorizer(\n",
        "  analyzer='word',\n",
        "  tokenizer=lambda x: str.split(x, sep=' '),\n",
        "  token_pattern=None,\n",
        "  lowercase=True,\n",
        "  stop_words=list(fr_stopwords),\n",
        "  n_features=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 678)\n",
            "   !  \"  (  )  -  ...  20  30  :  ;  ...  état  étrange  étranges  étudiant   \n",
            "0  0  0  0  0  1    0   0   0  2  0  ...     0        0         0         0  \\\n",
            "1  0  0  0  0  0    0   0   0  1  0  ...     0        0         0         0   \n",
            "2  0  0  0  0  0    0   0   0  0  0  ...     0        0         0         0   \n",
            "3  0  0  0  0  0    1   0   0  0  0  ...     0        0         0         0   \n",
            "4  0  0  0  0  1    0   0   0  0  0  ...     0        0         0         0   \n",
            "\n",
            "   événements  être  êtres  île  –  …  \n",
            "0           0     0      0    0  0  1  \n",
            "1           0     1      0    0  0  0  \n",
            "2           0     0      0    0  0  0  \n",
            "3           0     0      0    0  0  0  \n",
            "4           0     0      0    0  0  0  \n",
            "\n",
            "[5 rows x 678 columns]\n"
          ]
        }
      ],
      "source": [
        "res = count_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 1000)\n",
            "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5   \n",
            "0        0.0        0.0        0.0   0.000000   0.000000        0.0  \\\n",
            "1        0.0        0.0        0.0   0.000000  -0.108465        0.0   \n",
            "2        0.0        0.0        0.0   0.000000   0.000000        0.0   \n",
            "3        0.0        0.0        0.0  -0.152499   0.000000        0.0   \n",
            "4        0.0        0.0        0.0   0.000000   0.000000        0.0   \n",
            "\n",
            "   feature_6  feature_7  feature_8  feature_9  ...  feature_990  feature_991   \n",
            "0        0.0        0.0        0.0        0.0  ...          0.0          0.0  \\\n",
            "1        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "2        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "3        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "4        0.0        0.0        0.0        0.0  ...          0.0          0.0   \n",
            "\n",
            "   feature_992  feature_993  feature_994  feature_995  feature_996   \n",
            "0          0.0    -0.076923          0.0     0.000000          0.0  \\\n",
            "1          0.0     0.000000          0.0     0.000000          0.0   \n",
            "2          0.0     0.000000          0.0     0.000000          0.0   \n",
            "3          0.0     0.000000          0.0     0.152499          0.0   \n",
            "4          0.0     0.000000          0.0     0.000000          0.0   \n",
            "\n",
            "   feature_997  feature_998  feature_999  \n",
            "0     0.000000          0.0     0.000000  \n",
            "1     0.000000          0.0     0.000000  \n",
            "2     0.093659          0.0     0.093659  \n",
            "3     0.000000          0.0     0.000000  \n",
            "4     0.000000          0.0     0.000000  \n",
            "\n",
            "[5 rows x 1000 columns]\n"
          ]
        }
      ],
      "source": [
        "res = hashing_vectorizer.fit_transform(X_train['synopsis'])\n",
        "bow = pd.DataFrame(res.toarray(), columns=[f'feature_{i}' for i in range(1000)])\n",
        "\n",
        "print(bow.shape)\n",
        "print(bow.head())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Création de la pipeline\n",
        "\n",
        "Nous allons maintenant procéder à la création de la pipeline en combinant les chaînes de pré-traitement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_transformer = ColumnTransformer(\n",
        "  [\n",
        "    # 'synopsis' column : tf-idf vectorization\n",
        "    ('synopsis', tfidf_vectorizer, 'synopsis'),\n",
        "    # 'titre' column : tf-idf vectorization\n",
        "    ('titre', tfidf_vectorizer, 'titre'),\n",
        "\n",
        "    # 'synopsis' column : stats\n",
        "    ('synopsis_stats', Pipeline([\n",
        "      ('stats_transformer', stats_transformer),\n",
        "      ('stats_vectorizer', stats_vectorizer),\n",
        "      ('min_max_scaler', min_max_scaler),\n",
        "    ]), 'synopsis'),\n",
        "\n",
        "  ],\n",
        "  remainder='drop', # drop the columns not specified\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learning\n",
        "classifier_pipeline = make_pipeline(column_transformer, LogisticRegression(max_iter=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;synopsis&#x27;,\n",
              "                                                  TfidfVectorizer(max_df=0.95,\n",
              "                                                                  min_df=0.01,\n",
              "                                                                  stop_words=[&#x27;j&#x27;,\n",
              "                                                                              &#x27;serai&#x27;,\n",
              "                                                                              &#x27;seriez&#x27;,\n",
              "                                                                              &#x27;ayante&#x27;,\n",
              "                                                                              &#x27;les&#x27;,\n",
              "                                                                              &#x27;ces&#x27;,\n",
              "                                                                              &#x27;eurent&#x27;,\n",
              "                                                                              &#x27;pour&#x27;,\n",
              "                                                                              &#x27;ma&#x27;,\n",
              "                                                                              &#x27;est&#x27;,\n",
              "                                                                              &#x27;je&#x27;,\n",
              "                                                                              &#x27;me&#x27;,\n",
              "                                                                              &#x27;de&#x27;,\n",
              "                                                                              &#x27;se&#x27;,\n",
              "                                                                              &#x27;au&#x27;,\n",
              "                                                                              &#x27;ai&#x27;,\n",
              "                                                                              &#x27;avait&#x27;,\n",
              "                                                                              &#x27;aux&#x27;,\n",
              "                                                                              &#x27;soyez&#x27;,\n",
              "                                                                              &#x27;dans&#x27;,\n",
              "                                                                              &#x27;eusses&#x27;,\n",
              "                                                                              &#x27;par&#x27;,\n",
              "                                                                              &#x27;était&#x27;,\n",
              "                                                                              &#x27;c&#x27;,\n",
              "                                                                              &#x27;aurait&#x27;,\n",
              "                                                                              &#x27;te&#x27;,\n",
              "                                                                              &#x27;fus&#x27;,\n",
              "                                                                              &#x27;votre&#x27;,\n",
              "                                                                              &#x27;fût&#x27;,\n",
              "                                                                              &#x27;qu&#x27;, ...],\n",
              "                                                                  token_pattern=None...\n",
              "                                                                  token_pattern=None,\n",
              "                                                                  tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6b9f6e80&gt;),\n",
              "                                                  &#x27;titre&#x27;),\n",
              "                                                 (&#x27;synopsis_stats&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;stats_transformer&#x27;,\n",
              "                                                                   FunctionTransformer(func=&lt;function make_stats at 0x7fcf6b9f5260&gt;)),\n",
              "                                                                  (&#x27;stats_vectorizer&#x27;,\n",
              "                                                                   DictVectorizer(sparse=False)),\n",
              "                                                                  (&#x27;min_max_scaler&#x27;,\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  &#x27;synopsis&#x27;)])),\n",
              "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;synopsis&#x27;,\n",
              "                                                  TfidfVectorizer(max_df=0.95,\n",
              "                                                                  min_df=0.01,\n",
              "                                                                  stop_words=[&#x27;j&#x27;,\n",
              "                                                                              &#x27;serai&#x27;,\n",
              "                                                                              &#x27;seriez&#x27;,\n",
              "                                                                              &#x27;ayante&#x27;,\n",
              "                                                                              &#x27;les&#x27;,\n",
              "                                                                              &#x27;ces&#x27;,\n",
              "                                                                              &#x27;eurent&#x27;,\n",
              "                                                                              &#x27;pour&#x27;,\n",
              "                                                                              &#x27;ma&#x27;,\n",
              "                                                                              &#x27;est&#x27;,\n",
              "                                                                              &#x27;je&#x27;,\n",
              "                                                                              &#x27;me&#x27;,\n",
              "                                                                              &#x27;de&#x27;,\n",
              "                                                                              &#x27;se&#x27;,\n",
              "                                                                              &#x27;au&#x27;,\n",
              "                                                                              &#x27;ai&#x27;,\n",
              "                                                                              &#x27;avait&#x27;,\n",
              "                                                                              &#x27;aux&#x27;,\n",
              "                                                                              &#x27;soyez&#x27;,\n",
              "                                                                              &#x27;dans&#x27;,\n",
              "                                                                              &#x27;eusses&#x27;,\n",
              "                                                                              &#x27;par&#x27;,\n",
              "                                                                              &#x27;était&#x27;,\n",
              "                                                                              &#x27;c&#x27;,\n",
              "                                                                              &#x27;aurait&#x27;,\n",
              "                                                                              &#x27;te&#x27;,\n",
              "                                                                              &#x27;fus&#x27;,\n",
              "                                                                              &#x27;votre&#x27;,\n",
              "                                                                              &#x27;fût&#x27;,\n",
              "                                                                              &#x27;qu&#x27;, ...],\n",
              "                                                                  token_pattern=None...\n",
              "                                                                  token_pattern=None,\n",
              "                                                                  tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6b9f6e80&gt;),\n",
              "                                                  &#x27;titre&#x27;),\n",
              "                                                 (&#x27;synopsis_stats&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;stats_transformer&#x27;,\n",
              "                                                                   FunctionTransformer(func=&lt;function make_stats at 0x7fcf6b9f5260&gt;)),\n",
              "                                                                  (&#x27;stats_vectorizer&#x27;,\n",
              "                                                                   DictVectorizer(sparse=False)),\n",
              "                                                                  (&#x27;min_max_scaler&#x27;,\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  &#x27;synopsis&#x27;)])),\n",
              "                (&#x27;logisticregression&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;synopsis&#x27;,\n",
              "                                 TfidfVectorizer(max_df=0.95, min_df=0.01,\n",
              "                                                 stop_words=[&#x27;j&#x27;, &#x27;serai&#x27;,\n",
              "                                                             &#x27;seriez&#x27;, &#x27;ayante&#x27;,\n",
              "                                                             &#x27;les&#x27;, &#x27;ces&#x27;,\n",
              "                                                             &#x27;eurent&#x27;, &#x27;pour&#x27;,\n",
              "                                                             &#x27;ma&#x27;, &#x27;est&#x27;, &#x27;je&#x27;,\n",
              "                                                             &#x27;me&#x27;, &#x27;de&#x27;, &#x27;se&#x27;,\n",
              "                                                             &#x27;au&#x27;, &#x27;ai&#x27;,\n",
              "                                                             &#x27;avait&#x27;, &#x27;aux&#x27;,\n",
              "                                                             &#x27;soyez&#x27;, &#x27;dans&#x27;,\n",
              "                                                             &#x27;eusses&#x27;, &#x27;par&#x27;,\n",
              "                                                             &#x27;était&#x27;, &#x27;c&#x27;,\n",
              "                                                             &#x27;aurait&#x27;, &#x27;te&#x27;,\n",
              "                                                             &#x27;fus&#x27;, &#x27;votre&#x27;,\n",
              "                                                             &#x27;fût&#x27;, &#x27;qu&#x27;, ...],\n",
              "                                                 token_pattern=None,\n",
              "                                                 tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6...\n",
              "                                                             &#x27;eusses&#x27;, &#x27;par&#x27;,\n",
              "                                                             &#x27;était&#x27;, &#x27;c&#x27;,\n",
              "                                                             &#x27;aurait&#x27;, &#x27;te&#x27;,\n",
              "                                                             &#x27;fus&#x27;, &#x27;votre&#x27;,\n",
              "                                                             &#x27;fût&#x27;, &#x27;qu&#x27;, ...],\n",
              "                                                 token_pattern=None,\n",
              "                                                 tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6b9f6e80&gt;),\n",
              "                                 &#x27;titre&#x27;),\n",
              "                                (&#x27;synopsis_stats&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;stats_transformer&#x27;,\n",
              "                                                  FunctionTransformer(func=&lt;function make_stats at 0x7fcf6b9f5260&gt;)),\n",
              "                                                 (&#x27;stats_vectorizer&#x27;,\n",
              "                                                  DictVectorizer(sparse=False)),\n",
              "                                                 (&#x27;min_max_scaler&#x27;,\n",
              "                                                  MinMaxScaler())]),\n",
              "                                 &#x27;synopsis&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">synopsis</label><div class=\"sk-toggleable__content\"><pre>synopsis</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.95, min_df=0.01,\n",
              "                stop_words=[&#x27;j&#x27;, &#x27;serai&#x27;, &#x27;seriez&#x27;, &#x27;ayante&#x27;, &#x27;les&#x27;, &#x27;ces&#x27;,\n",
              "                            &#x27;eurent&#x27;, &#x27;pour&#x27;, &#x27;ma&#x27;, &#x27;est&#x27;, &#x27;je&#x27;, &#x27;me&#x27;, &#x27;de&#x27;,\n",
              "                            &#x27;se&#x27;, &#x27;au&#x27;, &#x27;ai&#x27;, &#x27;avait&#x27;, &#x27;aux&#x27;, &#x27;soyez&#x27;, &#x27;dans&#x27;,\n",
              "                            &#x27;eusses&#x27;, &#x27;par&#x27;, &#x27;était&#x27;, &#x27;c&#x27;, &#x27;aurait&#x27;, &#x27;te&#x27;,\n",
              "                            &#x27;fus&#x27;, &#x27;votre&#x27;, &#x27;fût&#x27;, &#x27;qu&#x27;, ...],\n",
              "                token_pattern=None,\n",
              "                tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6b9f6e80&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">titre</label><div class=\"sk-toggleable__content\"><pre>titre</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.95, min_df=0.01,\n",
              "                stop_words=[&#x27;j&#x27;, &#x27;serai&#x27;, &#x27;seriez&#x27;, &#x27;ayante&#x27;, &#x27;les&#x27;, &#x27;ces&#x27;,\n",
              "                            &#x27;eurent&#x27;, &#x27;pour&#x27;, &#x27;ma&#x27;, &#x27;est&#x27;, &#x27;je&#x27;, &#x27;me&#x27;, &#x27;de&#x27;,\n",
              "                            &#x27;se&#x27;, &#x27;au&#x27;, &#x27;ai&#x27;, &#x27;avait&#x27;, &#x27;aux&#x27;, &#x27;soyez&#x27;, &#x27;dans&#x27;,\n",
              "                            &#x27;eusses&#x27;, &#x27;par&#x27;, &#x27;était&#x27;, &#x27;c&#x27;, &#x27;aurait&#x27;, &#x27;te&#x27;,\n",
              "                            &#x27;fus&#x27;, &#x27;votre&#x27;, &#x27;fût&#x27;, &#x27;qu&#x27;, ...],\n",
              "                token_pattern=None,\n",
              "                tokenizer=&lt;function &lt;lambda&gt; at 0x7fcf6b9f6e80&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">synopsis_stats</label><div class=\"sk-toggleable__content\"><pre>synopsis</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function make_stats at 0x7fcf6b9f5260&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(sparse=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('columntransformer',\n",
              "                 ColumnTransformer(transformers=[('synopsis',\n",
              "                                                  TfidfVectorizer(max_df=0.95,\n",
              "                                                                  min_df=0.01,\n",
              "                                                                  stop_words=['j',\n",
              "                                                                              'serai',\n",
              "                                                                              'seriez',\n",
              "                                                                              'ayante',\n",
              "                                                                              'les',\n",
              "                                                                              'ces',\n",
              "                                                                              'eurent',\n",
              "                                                                              'pour',\n",
              "                                                                              'ma',\n",
              "                                                                              'est',\n",
              "                                                                              'je',\n",
              "                                                                              'me',\n",
              "                                                                              'de',\n",
              "                                                                              'se',\n",
              "                                                                              'au',\n",
              "                                                                              'ai',\n",
              "                                                                              'avait',\n",
              "                                                                              'aux',\n",
              "                                                                              'soyez',\n",
              "                                                                              'dans',\n",
              "                                                                              'eusses',\n",
              "                                                                              'par',\n",
              "                                                                              'était',\n",
              "                                                                              'c',\n",
              "                                                                              'aurait',\n",
              "                                                                              'te',\n",
              "                                                                              'fus',\n",
              "                                                                              'votre',\n",
              "                                                                              'fût',\n",
              "                                                                              'qu', ...],\n",
              "                                                                  token_pattern=None...\n",
              "                                                                  token_pattern=None,\n",
              "                                                                  tokenizer=<function <lambda> at 0x7fcf6b9f6e80>),\n",
              "                                                  'titre'),\n",
              "                                                 ('synopsis_stats',\n",
              "                                                  Pipeline(steps=[('stats_transformer',\n",
              "                                                                   FunctionTransformer(func=<function make_stats at 0x7fcf6b9f5260>)),\n",
              "                                                                  ('stats_vectorizer',\n",
              "                                                                   DictVectorizer(sparse=False)),\n",
              "                                                                  ('min_max_scaler',\n",
              "                                                                   MinMaxScaler())]),\n",
              "                                                  'synopsis')])),\n",
              "                ('logisticregression', LogisticRegression(max_iter=1000))])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit\n",
        "classifier_pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         biopic       0.39      0.23      0.29        47\n",
            "        comédie       0.46      0.58      0.51       116\n",
            "   documentaire       0.52      0.46      0.49        37\n",
            "          drame       0.36      0.50      0.41       117\n",
            "     historique       0.59      0.20      0.30        50\n",
            "        horreur       0.52      0.38      0.44        84\n",
            "       policier       0.54      0.60      0.57        75\n",
            "        romance       0.39      0.39      0.39       110\n",
            "science fiction       0.66      0.63      0.64        83\n",
            "\n",
            "       accuracy                           0.47       719\n",
            "      macro avg       0.49      0.44      0.45       719\n",
            "   weighted avg       0.48      0.47      0.46       719\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# predict\n",
        "test: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_test.csv'))\n",
        "\n",
        "X_test = test[values]\n",
        "y_test = test['genre']\n",
        "\n",
        "y_pred = classifier_pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validation croisée\n",
        "\n",
        "Nous allons maintenant procéder à la validation croisée de notre modèle. Nous allons utiliser `GridSearchCV` pour tester plusieurs hyperparamètres et choisir les meilleurs.\n",
        "\n",
        "Les imports nécessaires sont effectués dans la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline\n",
            "  0.033 +/- 0.000\n",
            "Mutinomial NB\n",
            "  0.313 +/- 0.018\n",
            "CART\n",
            "  0.241 +/- 0.015\n",
            "LR\n",
            "  0.405 +/- 0.034\n",
            "KNN\n",
            "  0.173 +/- 0.024\n",
            "Random forest\n",
            "  0.360 +/- 0.029\n",
            "{'logisticregression__C': 1, 'logisticregression__solver': 'lbfgs'}\n",
            "0.40512052996742887\n",
            "Pipeline(steps=[('columntransformer',\n",
            "                 ColumnTransformer(transformers=[('synopsis',\n",
            "                                                  TfidfVectorizer(max_df=0.95,\n",
            "                                                                  min_df=0.01,\n",
            "                                                                  stop_words=['j',\n",
            "                                                                              'serai',\n",
            "                                                                              'seriez',\n",
            "                                                                              'ayante',\n",
            "                                                                              'les',\n",
            "                                                                              'ces',\n",
            "                                                                              'eurent',\n",
            "                                                                              'pour',\n",
            "                                                                              'ma',\n",
            "                                                                              'est',\n",
            "                                                                              'je',\n",
            "                                                                              'me',\n",
            "                                                                              'de',\n",
            "                                                                              'se',\n",
            "                                                                              'au',\n",
            "                                                                              'ai',\n",
            "                                                                              'avait',\n",
            "                                                                              'aux',\n",
            "                                                                              'soyez',\n",
            "                                                                              'dans',\n",
            "                                                                              'eusses',\n",
            "                                                                              'par',\n",
            "                                                                              'était',\n",
            "                                                                              'c',\n",
            "                                                                              'aurait',\n",
            "                                                                              'te',\n",
            "                                                                              'fus',\n",
            "                                                                              'votre',\n",
            "                                                                              'fût',\n",
            "                                                                              'qu', ...],\n",
            "                                                                  token_pattern=None...\n",
            "                                                                  token_pattern=None,\n",
            "                                                                  tokenizer=<function <lambda> at 0x7fcf6b9f6e80>),\n",
            "                                                  'titre'),\n",
            "                                                 ('synopsis_stats',\n",
            "                                                  Pipeline(steps=[('stats_transformer',\n",
            "                                                                   FunctionTransformer(func=<function make_stats at 0x7fcf6b9f5260>)),\n",
            "                                                                  ('stats_vectorizer',\n",
            "                                                                   DictVectorizer(sparse=False)),\n",
            "                                                                  ('min_max_scaler',\n",
            "                                                                   MinMaxScaler())]),\n",
            "                                                  'synopsis')])),\n",
            "                ('logisticregression', LogisticRegression(C=1, max_iter=1000))])\n",
            "{'mean_fit_time': array([0.11509118, 0.09615111, 0.17889214, 0.11240377, 0.35584502,\n",
            "       0.13759079]), 'std_fit_time': array([0.00517653, 0.0036521 , 0.00212857, 0.0051161 , 0.06725735,\n",
            "       0.00328727]), 'mean_score_time': array([0.0203711 , 0.02169051, 0.02114582, 0.02040162, 0.02099004,\n",
            "       0.02225633]), 'std_score_time': array([0.00099368, 0.00150738, 0.00110676, 0.00145217, 0.00062614,\n",
            "       0.00184257]), 'param_logisticregression__C': masked_array(data=[0.1, 0.1, 1, 1, 10, 10],\n",
            "             mask=[False, False, False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_logisticregression__solver': masked_array(data=['lbfgs', 'liblinear', 'lbfgs', 'liblinear', 'lbfgs',\n",
            "                   'liblinear'],\n",
            "             mask=[False, False, False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'logisticregression__C': 0.1, 'logisticregression__solver': 'lbfgs'}, {'logisticregression__C': 0.1, 'logisticregression__solver': 'liblinear'}, {'logisticregression__C': 1, 'logisticregression__solver': 'lbfgs'}, {'logisticregression__C': 1, 'logisticregression__solver': 'liblinear'}, {'logisticregression__C': 10, 'logisticregression__solver': 'lbfgs'}, {'logisticregression__C': 10, 'logisticregression__solver': 'liblinear'}], 'split0_test_score': array([0.20155445, 0.19603019, 0.41502487, 0.41800856, 0.4076154 ,\n",
            "       0.40391429]), 'split1_test_score': array([0.21205631, 0.2014612 , 0.3921506 , 0.38503333, 0.39460108,\n",
            "       0.39588923]), 'split2_test_score': array([0.19403508, 0.18389054, 0.39900204, 0.39259479, 0.37726913,\n",
            "       0.38635902]), 'split3_test_score': array([0.22276214, 0.21680143, 0.35818137, 0.35057171, 0.34029071,\n",
            "       0.34768765]), 'split4_test_score': array([0.22759617, 0.22154833, 0.46124377, 0.44939875, 0.39972329,\n",
            "       0.42535507]), 'mean_test_score': array([0.21160083, 0.20394634, 0.40512053, 0.39912143, 0.38389992,\n",
            "       0.39184105]), 'std_test_score': array([0.01257075, 0.01375616, 0.03363586, 0.03311398, 0.02397102,\n",
            "       0.02555353]), 'rank_test_score': array([5, 6, 1, 2, 4, 3], dtype=int32)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "models = [\n",
        "  ('Baseline', DummyClassifier(strategy='most_frequent')),\n",
        "  ('Mutinomial NB', MultinomialNB()),\n",
        "  ('CART', DecisionTreeClassifier()),\n",
        "  ('LR', LogisticRegression()),\n",
        "  ('KNN', KNeighborsClassifier()),\n",
        "  ('Random forest', RandomForestClassifier()),\n",
        "]\n",
        "\n",
        "# do cross validation\n",
        "for name, model in models:\n",
        "  print(name)\n",
        "  pipeline = make_pipeline(column_transformer, model)\n",
        "  scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1_macro')\n",
        "  print(f'  {scores.mean():.3f} +/- {scores.std():.3f}')\n",
        "\n",
        "# do grid search on the best model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "  'logisticregression__C': [0.1, 1, 10],\n",
        "  'logisticregression__solver': ['lbfgs', 'liblinear'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(classifier_pipeline, param_grid, cv=5, scoring='f1_macro')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "print(grid_search.cv_results_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# augmented file with best classifier\n",
        "test: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_test.csv'))\n",
        "new_test = test.copy()\n",
        "num_cols = new_test.shape[1]\n",
        "\n",
        "# add genre_predicted column\n",
        "new_test.insert(num_cols, 'genre_predit', y_pred)\n",
        "\n",
        "# add genre_predicted_proba column\n",
        "y_pred_proba = classifier_pipeline.predict_proba(X_test)\n",
        "new_test.insert(num_cols + 1, 'genre_predit_proba', y_pred_proba.max(axis=1))\n",
        "\n",
        "# write the new file\n",
        "new_test.to_csv(os.path.join('data', 'allocine_genres_test_augmented.csv'), index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Passage sur les transformers\n",
        "\n",
        "Nous allons maintenant explorer la piste des transformers. Pour des raisons de reproductibilité, nous allons utiliser des modèles pré-entraînés. Tous les imports nécessaires seront effectués sur les cellules suivantes.\n",
        "\n",
        "Nous allons utiliser le modèle `xlm-roberta-large-xnli` pour la classification de nos données. Nous avons besoin pour cela de nouveaux packages :\n",
        "\n",
        "- `transformers`\n",
        "- `torch`\n",
        "- `protobuf`\n",
        "- `sentencepiece`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tbyr/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "uri = 'BaptisteDoyen/camembert-base-xnli'\n",
        "tokenizer = AutoTokenizer.from_pretrained(uri)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(uri).to(device)\n",
        "\n",
        "train: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_train.csv'))\n",
        "\n",
        "classifier = pipeline('zero-shot-classification', model=model, tokenizer=tokenizer)\n",
        "\n",
        "candidate_labels: list[str] = train['genre'].unique().tolist()\n",
        "hypothesis_template: str = \"Ce film est du genre {}.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_genre(text: str) -> str:\n",
        "  result = classifier(text, candidate_labels, hypothesis_template=hypothesis_template)\n",
        "  return result['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875, 22)\n"
          ]
        }
      ],
      "source": [
        "EMPTY_TOKEN = '<EMPTY>'\n",
        "\n",
        "# replace missing values with either the mean or the median (or empty token)\n",
        "list_headers = train.columns.values.tolist()\n",
        "list_of_numerical_headers = train._get_numeric_data().columns.values.tolist()\n",
        "list_of_categorical_headers = list(set(list_headers) - set(list_of_numerical_headers))\n",
        "\n",
        "train_dataset_2 = train.copy()\n",
        "for header in list_of_numerical_headers:\n",
        "  train_dataset_2[header].fillna(train_dataset_2[header].median(), inplace=True)\n",
        "for header in list_of_categorical_headers:\n",
        "  train_dataset_2[header].fillna(EMPTY_TOKEN, inplace=True)\n",
        "\n",
        "print(train_dataset_2.shape)\n",
        "X_train, y_train = train_dataset_2['synopsis'], train_dataset_2['genre']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2875,)\n",
            "(719,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 719/719 [09:08<00:00,  1.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# make model\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tqdm\n",
        "\n",
        "class GenrePredictor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, classifier):\n",
        "    self.classifier = classifier # pipeline\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    return self # nothing else to do\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [predict_genre(x) for x in tqdm.tqdm(X)]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir='./results',          # output directory\n",
        "  num_train_epochs=1,              # total number of training epochs\n",
        "  per_device_train_batch_size=16,  # batch size per device during training\n",
        "  per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "  weight_decay=0.01,               # strength of weight decay\n",
        "  logging_dir='./logs',            # directory for storing logs\n",
        "  logging_steps=10,\n",
        ")\n",
        "\n",
        "# todo: make train and eval datasets\n",
        "trainer = Trainer(\n",
        "  model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "  args=training_args,                  # training arguments, defined above\n",
        "  train_dataset=train_dataset_2,       # training dataset\n",
        "  eval_dataset=train_dataset_2,        # evaluation dataset\n",
        ")\n",
        "\n",
        "# learning\n",
        "classifier_pipeline = make_pipeline(GenrePredictor(classifier))\n",
        "\n",
        "# fit\n",
        "print(X_train.shape)\n",
        "classifier_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "test: pd.DataFrame = pd.read_csv(os.path.join('data', 'allocine_genres_test.csv'))\n",
        "\n",
        "X_test = test['synopsis']\n",
        "y_test = test['genre']\n",
        "\n",
        "print(X_test.shape)\n",
        "y_pred = classifier_pipeline.predict(X_test) # takes about 8 minutes (i9-13900h, 32GB RAM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0      Gilbert Grape vit à Endora dans l' Iowa , avec...\n",
            "1      Aventure à la fois complexe et mystérieuse sur...\n",
            "2      A la mort de sa mère , Anne fait une découvert...\n",
            "3      Christiane , une jeune berlinoise de treize an...\n",
            "4      Après son voyage mouvementé entre passé , prés...\n",
            "                             ...                        \n",
            "714    Un jeune ténor du barreau gagne toutes les cau...\n",
            "715    Un biopic du Président des Etats-Unis Theodore...\n",
            "716    Deux astronautes , le Lieutenant Payton et le ...\n",
            "717    L' histoire vraie d' une avocate défendant une...\n",
            "718    A Casablanca , pendant la Seconde Guerre mondi...\n",
            "Name: synopsis, Length: 719, dtype: object\n",
            "['historique', 'historique', 'historique', 'drame', 'historique', 'drame', 'drame', 'policier', 'romance', 'comédie', 'historique', 'historique', 'drame', 'comédie', 'historique', 'policier', 'historique', 'policier', 'policier', 'drame', 'historique', 'documentaire', 'horreur', 'historique', 'comédie', 'policier', 'drame', 'policier', 'historique', 'historique', 'historique', 'historique', 'romance', 'historique', 'romance', 'historique', 'horreur', 'historique', 'historique', 'historique', 'historique', 'romance', 'drame', 'historique', 'drame', 'drame', 'romance', 'historique', 'policier', 'policier', 'documentaire', 'comédie', 'drame', 'historique', 'drame', 'historique', 'drame', 'romance', 'policier', 'policier', 'historique', 'historique', 'drame', 'historique', 'horreur', 'drame', 'drame', 'drame', 'historique', 'policier', 'policier', 'historique', 'horreur', 'historique', 'drame', 'horreur', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'policier', 'documentaire', 'drame', 'policier', 'policier', 'horreur', 'romance', 'comédie', 'policier', 'historique', 'historique', 'policier', 'comédie', 'romance', 'comédie', 'historique', 'historique', 'comédie', 'documentaire', 'horreur', 'historique', 'drame', 'drame', 'drame', 'documentaire', 'historique', 'historique', 'horreur', 'historique', 'documentaire', 'drame', 'drame', 'historique', 'drame', 'policier', 'biopic', 'historique', 'historique', 'comédie', 'drame', 'historique', 'romance', 'policier', 'policier', 'drame', 'historique', 'policier', 'policier', 'historique', 'documentaire', 'drame', 'historique', 'comédie', 'drame', 'documentaire', 'romance', 'policier', 'science fiction', 'policier', 'science fiction', 'drame', 'drame', 'historique', 'drame', 'policier', 'comédie', 'drame', 'historique', 'policier', 'policier', 'romance', 'historique', 'historique', 'historique', 'documentaire', 'historique', 'policier', 'historique', 'policier', 'documentaire', 'drame', 'drame', 'horreur', 'horreur', 'romance', 'historique', 'historique', 'romance', 'historique', 'romance', 'horreur', 'horreur', 'policier', 'horreur', 'policier', 'drame', 'documentaire', 'drame', 'historique', 'historique', 'science fiction', 'drame', 'policier', 'historique', 'historique', 'policier', 'drame', 'drame', 'historique', 'policier', 'historique', 'drame', 'horreur', 'documentaire', 'policier', 'drame', 'drame', 'historique', 'historique', 'horreur', 'policier', 'historique', 'historique', 'historique', 'historique', 'policier', 'drame', 'policier', 'historique', 'documentaire', 'romance', 'drame', 'historique', 'science fiction', 'comédie', 'historique', 'documentaire', 'horreur', 'historique', 'historique', 'drame', 'historique', 'drame', 'historique', 'policier', 'documentaire', 'historique', 'documentaire', 'horreur', 'drame', 'historique', 'historique', 'documentaire', 'historique', 'horreur', 'policier', 'historique', 'historique', 'policier', 'romance', 'comédie', 'drame', 'documentaire', 'historique', 'documentaire', 'policier', 'historique', 'policier', 'policier', 'drame', 'policier', 'drame', 'historique', 'horreur', 'policier', 'documentaire', 'historique', 'historique', 'drame', 'drame', 'historique', 'historique', 'comédie', 'historique', 'historique', 'comédie', 'drame', 'historique', 'comédie', 'historique', 'policier', 'comédie', 'documentaire', 'drame', 'historique', 'historique', 'historique', 'documentaire', 'documentaire', 'documentaire', 'horreur', 'documentaire', 'documentaire', 'historique', 'drame', 'historique', 'documentaire', 'historique', 'drame', 'documentaire', 'historique', 'romance', 'policier', 'historique', 'horreur', 'historique', 'science fiction', 'science fiction', 'historique', 'historique', 'romance', 'documentaire', 'policier', 'historique', 'drame', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'drame', 'romance', 'drame', 'drame', 'drame', 'policier', 'drame', 'documentaire', 'documentaire', 'drame', 'historique', 'drame', 'documentaire', 'policier', 'historique', 'drame', 'policier', 'historique', 'drame', 'romance', 'horreur', 'documentaire', 'historique', 'historique', 'horreur', 'horreur', 'policier', 'drame', 'romance', 'historique', 'historique', 'drame', 'historique', 'historique', 'historique', 'drame', 'horreur', 'drame', 'historique', 'drame', 'drame', 'romance', 'historique', 'policier', 'historique', 'historique', 'policier', 'drame', 'historique', 'horreur', 'historique', 'horreur', 'documentaire', 'documentaire', 'drame', 'historique', 'historique', 'drame', 'romance', 'documentaire', 'science fiction', 'historique', 'drame', 'documentaire', 'historique', 'historique', 'drame', 'romance', 'historique', 'documentaire', 'horreur', 'policier', 'policier', 'horreur', 'historique', 'drame', 'drame', 'comédie', 'drame', 'drame', 'drame', 'drame', 'policier', 'historique', 'policier', 'documentaire', 'documentaire', 'historique', 'romance', 'historique', 'horreur', 'historique', 'drame', 'drame', 'historique', 'policier', 'drame', 'policier', 'documentaire', 'policier', 'comédie', 'documentaire', 'drame', 'policier', 'romance', 'historique', 'drame', 'historique', 'drame', 'comédie', 'drame', 'drame', 'drame', 'romance', 'drame', 'romance', 'drame', 'drame', 'romance', 'romance', 'documentaire', 'horreur', 'documentaire', 'drame', 'historique', 'historique', 'historique', 'historique', 'historique', 'documentaire', 'policier', 'historique', 'policier', 'historique', 'historique', 'drame', 'drame', 'drame', 'drame', 'policier', 'historique', 'historique', 'comédie', 'historique', 'horreur', 'romance', 'historique', 'drame', 'policier', 'policier', 'drame', 'policier', 'drame', 'policier', 'horreur', 'historique', 'drame', 'drame', 'drame', 'documentaire', 'historique', 'historique', 'comédie', 'historique', 'policier', 'historique', 'documentaire', 'documentaire', 'drame', 'romance', 'drame', 'drame', 'comédie', 'drame', 'documentaire', 'drame', 'romance', 'drame', 'documentaire', 'policier', 'historique', 'historique', 'policier', 'drame', 'historique', 'horreur', 'drame', 'historique', 'historique', 'historique', 'historique', 'drame', 'historique', 'drame', 'historique', 'historique', 'policier', 'documentaire', 'horreur', 'drame', 'romance', 'documentaire', 'policier', 'historique', 'drame', 'drame', 'historique', 'documentaire', 'comédie', 'policier', 'comédie', 'historique', 'drame', 'historique', 'comédie', 'policier', 'drame', 'comédie', 'historique', 'drame', 'documentaire', 'horreur', 'comédie', 'drame', 'drame', 'science fiction', 'documentaire', 'historique', 'historique', 'drame', 'romance', 'comédie', 'historique', 'historique', 'drame', 'drame', 'romance', 'policier', 'policier', 'drame', 'historique', 'policier', 'documentaire', 'horreur', 'romance', 'romance', 'drame', 'policier', 'drame', 'drame', 'policier', 'drame', 'historique', 'policier', 'historique', 'historique', 'documentaire', 'policier', 'historique', 'romance', 'historique', 'documentaire', 'historique', 'drame', 'documentaire', 'documentaire', 'documentaire', 'historique', 'drame', 'drame', 'historique', 'documentaire', 'documentaire', 'documentaire', 'historique', 'documentaire', 'drame', 'policier', 'documentaire', 'drame', 'drame', 'documentaire', 'drame', 'historique', 'drame', 'historique', 'policier', 'comédie', 'drame', 'historique', 'romance', 'historique', 'historique', 'romance', 'historique', 'historique', 'documentaire', 'drame', 'drame', 'drame', 'policier', 'drame', 'drame', 'drame', 'historique', 'drame', 'documentaire', 'historique', 'comédie', 'drame', 'historique', 'drame', 'comédie', 'romance', 'historique', 'historique', 'historique', 'policier', 'drame', 'documentaire', 'drame', 'historique', 'comédie', 'historique', 'romance', 'drame', 'horreur', 'comédie', 'historique', 'drame', 'documentaire', 'horreur', 'drame', 'historique', 'drame', 'historique', 'historique', 'historique', 'documentaire', 'historique', 'historique', 'policier', 'drame', 'policier', 'policier', 'documentaire', 'policier', 'comédie', 'documentaire', 'romance', 'comédie', 'drame', 'historique', 'documentaire', 'documentaire', 'policier', 'drame', 'historique', 'policier', 'policier', 'historique', 'historique', 'historique', 'comédie', 'historique', 'documentaire', 'historique', 'romance', 'policier', 'historique', 'historique', 'comédie', 'science fiction', 'historique', 'romance', 'romance', 'drame', 'historique', 'drame', 'historique', 'science fiction', 'historique', 'biopic', 'drame', 'documentaire', 'documentaire', 'historique', 'historique', 'historique', 'drame', 'policier', 'science fiction', 'documentaire', 'biopic', 'historique', 'drame', 'drame', 'drame', 'romance', 'historique', 'comédie', 'drame', 'biopic', 'horreur', 'historique', 'historique']\n",
            "['romance', 'science fiction', 'romance', 'biopic', 'science fiction', 'horreur', 'romance', 'drame', 'romance', 'romance', 'historique', 'policier', 'science fiction', 'biopic', 'documentaire', 'drame', 'comédie', 'historique', 'horreur', 'drame', 'science fiction', 'drame', 'horreur', 'science fiction', 'comédie', 'drame', 'drame', 'policier', 'science fiction', 'biopic', 'romance', 'comédie', 'comédie', 'drame', 'drame', 'documentaire', 'documentaire', 'biopic', 'drame', 'historique', 'drame', 'romance', 'horreur', 'biopic', 'drame', 'biopic', 'romance', 'comédie', 'policier', 'policier', 'policier', 'comédie', 'drame', 'historique', 'horreur', 'biopic', 'comédie', 'romance', 'policier', 'policier', 'science fiction', 'documentaire', 'romance', 'historique', 'horreur', 'science fiction', 'drame', 'drame', 'drame', 'policier', 'policier', 'historique', 'policier', 'policier', 'horreur', 'horreur', 'romance', 'historique', 'comédie', 'romance', 'policier', 'horreur', 'policier', 'documentaire', 'comédie', 'policier', 'science fiction', 'horreur', 'comédie', 'comédie', 'historique', 'drame', 'romance', 'biopic', 'historique', 'drame', 'romance', 'romance', 'historique', 'drame', 'horreur', 'science fiction', 'romance', 'comédie', 'drame', 'policier', 'comédie', 'drame', 'drame', 'horreur', 'documentaire', 'horreur', 'romance', 'comédie', 'romance', 'science fiction', 'science fiction', 'comédie', 'drame', 'drame', 'comédie', 'comédie', 'romance', 'drame', 'policier', 'biopic', 'drame', 'drame', 'comédie', 'policier', 'horreur', 'comédie', 'drame', 'romance', 'drame', 'science fiction', 'documentaire', 'romance', 'drame', 'science fiction', 'policier', 'horreur', 'comédie', 'science fiction', 'science fiction', 'comédie', 'horreur', 'comédie', 'romance', 'biopic', 'romance', 'policier', 'drame', 'documentaire', 'comédie', 'historique', 'romance', 'drame', 'drame', 'comédie', 'romance', 'romance', 'science fiction', 'comédie', 'horreur', 'horreur', 'romance', 'drame', 'horreur', 'romance', 'drame', 'romance', 'horreur', 'horreur', 'policier', 'horreur', 'policier', 'policier', 'drame', 'science fiction', 'drame', 'biopic', 'science fiction', 'science fiction', 'policier', 'drame', 'comédie', 'romance', 'comédie', 'drame', 'historique', 'policier', 'science fiction', 'comédie', 'science fiction', 'science fiction', 'science fiction', 'drame', 'drame', 'romance', 'romance', 'horreur', 'drame', 'drame', 'drame', 'romance', 'historique', 'policier', 'comédie', 'policier', 'romance', 'romance', 'romance', 'romance', 'science fiction', 'science fiction', 'romance', 'drame', 'horreur', 'horreur', 'historique', 'horreur', 'policier', 'science fiction', 'science fiction', 'horreur', 'horreur', 'policier', 'biopic', 'drame', 'science fiction', 'science fiction', 'historique', 'drame', 'documentaire', 'horreur', 'romance', 'policier', 'science fiction', 'comédie', 'comédie', 'romance', 'romance', 'documentaire', 'comédie', 'biopic', 'historique', 'drame', 'horreur', 'policier', 'policier', 'horreur', 'horreur', 'horreur', 'romance', 'horreur', 'policier', 'science fiction', 'comédie', 'historique', 'comédie', 'drame', 'romance', 'horreur', 'romance', 'historique', 'biopic', 'comédie', 'comédie', 'science fiction', 'horreur', 'horreur', 'policier', 'horreur', 'drame', 'policier', 'science fiction', 'documentaire', 'science fiction', 'drame', 'biopic', 'documentaire', 'policier', 'documentaire', 'comédie', 'historique', 'horreur', 'documentaire', 'comédie', 'science fiction', 'comédie', 'horreur', 'horreur', 'comédie', 'biopic', 'documentaire', 'horreur', 'drame', 'comédie', 'horreur', 'comédie', 'comédie', 'romance', 'drame', 'policier', 'drame', 'comédie', 'drame', 'drame', 'drame', 'drame', 'comédie', 'historique', 'romance', 'drame', 'horreur', 'romance', 'comédie', 'policier', 'science fiction', 'comédie', 'science fiction', 'biopic', 'biopic', 'science fiction', 'documentaire', 'policier', 'historique', 'comédie', 'policier', 'comédie', 'policier', 'romance', 'policier', 'drame', 'horreur', 'drame', 'horreur', 'horreur', 'drame', 'horreur', 'romance', 'drame', 'historique', 'drame', 'romance', 'historique', 'science fiction', 'policier', 'science fiction', 'comédie', 'biopic', 'drame', 'science fiction', 'romance', 'comédie', 'science fiction', 'drame', 'documentaire', 'policier', 'horreur', 'drame', 'policier', 'science fiction', 'science fiction', 'biopic', 'historique', 'romance', 'documentaire', 'historique', 'horreur', 'romance', 'romance', 'comédie', 'biopic', 'romance', 'drame', 'horreur', 'biopic', 'drame', 'romance', 'historique', 'comédie', 'historique', 'biopic', 'policier', 'horreur', 'historique', 'romance', 'policier', 'comédie', 'science fiction', 'drame', 'romance', 'comédie', 'policier', 'biopic', 'horreur', 'biopic', 'comédie', 'horreur', 'comédie', 'historique', 'horreur', 'biopic', 'drame', 'romance', 'horreur', 'comédie', 'horreur', 'policier', 'drame', 'policier', 'romance', 'documentaire', 'horreur', 'drame', 'romance', 'romance', 'comédie', 'historique', 'science fiction', 'romance', 'comédie', 'drame', 'science fiction', 'romance', 'drame', 'comédie', 'romance', 'romance', 'comédie', 'drame', 'drame', 'romance', 'documentaire', 'documentaire', 'documentaire', 'science fiction', 'romance', 'drame', 'biopic', 'historique', 'comédie', 'drame', 'comédie', 'romance', 'science fiction', 'drame', 'science fiction', 'horreur', 'horreur', 'policier', 'drame', 'biopic', 'comédie', 'romance', 'horreur', 'drame', 'historique', 'drame', 'policier', 'drame', 'comédie', 'policier', 'comédie', 'horreur', 'comédie', 'romance', 'policier', 'biopic', 'romance', 'historique', 'romance', 'historique', 'biopic', 'biopic', 'policier', 'science fiction', 'historique', 'comédie', 'science fiction', 'drame', 'romance', 'science fiction', 'comédie', 'romance', 'policier', 'documentaire', 'romance', 'romance', 'documentaire', 'horreur', 'science fiction', 'horreur', 'policier', 'drame', 'historique', 'horreur', 'comédie', 'drame', 'biopic', 'science fiction', 'biopic', 'drame', 'horreur', 'drame', 'drame', 'biopic', 'romance', 'documentaire', 'horreur', 'biopic', 'romance', 'documentaire', 'policier', 'comédie', 'romance', 'comédie', 'historique', 'drame', 'comédie', 'science fiction', 'drame', 'comédie', 'science fiction', 'romance', 'romance', 'policier', 'comédie', 'comédie', 'policier', 'drame', 'documentaire', 'horreur', 'comédie', 'horreur', 'horreur', 'horreur', 'romance', 'drame', 'comédie', 'historique', 'romance', 'romance', 'biopic', 'science fiction', 'drame', 'science fiction', 'romance', 'comédie', 'policier', 'comédie', 'policier', 'drame', 'documentaire', 'horreur', 'comédie', 'comédie', 'science fiction', 'comédie', 'drame', 'comédie', 'science fiction', 'horreur', 'historique', 'policier', 'science fiction', 'science fiction', 'comédie', 'historique', 'science fiction', 'biopic', 'science fiction', 'comédie', 'science fiction', 'horreur', 'biopic', 'romance', 'romance', 'science fiction', 'comédie', 'biopic', 'drame', 'drame', 'documentaire', 'comédie', 'documentaire', 'drame', 'comédie', 'drame', 'comédie', 'comédie', 'romance', 'biopic', 'comédie', 'comédie', 'science fiction', 'comédie', 'policier', 'drame', 'horreur', 'historique', 'policier', 'romance', 'drame', 'comédie', 'drame', 'biopic', 'horreur', 'comédie', 'horreur', 'drame', 'policier', 'policier', 'comédie', 'comédie', 'science fiction', 'comédie', 'romance', 'science fiction', 'drame', 'historique', 'science fiction', 'documentaire', 'comédie', 'comédie', 'romance', 'drame', 'historique', 'policier', 'science fiction', 'biopic', 'romance', 'comédie', 'comédie', 'drame', 'drame', 'romance', 'comédie', 'romance', 'science fiction', 'romance', 'documentaire', 'horreur', 'science fiction', 'biopic', 'science fiction', 'drame', 'documentaire', 'drame', 'horreur', 'biopic', 'romance', 'policier', 'documentaire', 'policier', 'science fiction', 'romance', 'policier', 'comédie', 'horreur', 'comédie', 'romance', 'romance', 'drame', 'drame', 'horreur', 'comédie', 'policier', 'horreur', 'policier', 'policier', 'historique', 'science fiction', 'policier', 'comédie', 'drame', 'comédie', 'historique', 'romance', 'policier', 'science fiction', 'science fiction', 'romance', 'documentaire', 'horreur', 'comédie', 'comédie', 'historique', 'drame', 'drame', 'historique', 'comédie', 'romance', 'historique', 'science fiction', 'romance', 'romance', 'historique', 'historique', 'comédie', 'science fiction', 'drame', 'science fiction', 'documentaire', 'comédie', 'romance', 'drame', 'documentaire', 'romance', 'romance', 'biopic', 'romance', 'comédie', 'biopic', 'horreur', 'biopic', 'romance']\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         biopic       0.25      0.02      0.04        47\n",
            "        comédie       0.41      0.14      0.21       116\n",
            "   documentaire       0.20      0.43      0.27        37\n",
            "          drame       0.19      0.28      0.23       117\n",
            "     historique       0.16      0.72      0.26        50\n",
            "        horreur       0.62      0.30      0.40        84\n",
            "       policier       0.53      0.68      0.59        75\n",
            "        romance       0.50      0.22      0.30       110\n",
            "science fiction       0.36      0.05      0.09        83\n",
            "\n",
            "       accuracy                           0.29       719\n",
            "      macro avg       0.36      0.32      0.27       719\n",
            "   weighted avg       0.38      0.29      0.27       719\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(X_test)\n",
        "print(y_pred)\n",
        "print(y_test.to_list())\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
